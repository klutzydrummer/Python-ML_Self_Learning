{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1eXoy24gkH9f-1ARqCuKG5wS2vcjAdVpb",
      "authorship_tag": "ABX9TyPmYaKmE1ymVL9y6wPD1zNi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/klutzydrummer/Python_Projects/blob/main/article_analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pip\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "import dataclasses\n",
        "import uuid\n",
        "import datetime\n",
        "warnings.filterwarnings(\"ignore\", message=\"Setuptools is replacing distutils.\")\n",
        "\n",
        "pyrequirements_path = Path(\"requirements.txt\")\n",
        "\n",
        "if pyrequirements_path.exists() is not True:\n",
        "    with open(pyrequirements_path, \"w\") as project_file:\n",
        "        project_file.write('''numpy\n",
        "    pymysql\n",
        "    nltk\n",
        "    pyarrow\n",
        "    xformers\n",
        "    transformers[torch]\n",
        "    sentence_transformers\n",
        "    setfit\n",
        "    aiomysql\n",
        "    pyyaml >= 6.0.1\n",
        "    tqdm >= 4.66.0\n",
        "    asyncio >= 3.4.3\n",
        "    pendulum >= 2.1.2\n",
        "    pandas >= 2.0.3\n",
        "    psycopg2-binary >= 2.9.7''')\n",
        "\n",
        "    pip.main([\"install\", \"-r\", str(pyrequirements_path)])\n",
        "\n",
        "from pathlib import Path as path\n",
        "\n",
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "import aiomysql\n",
        "import pandas as pd\n",
        "import asyncio\n",
        "import ssl\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import List\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(init=True, repr=True, eq=True)\n",
        "class SentimentScore:\n",
        "    article_id: uuid.UUID\n",
        "    stock_symbol: str\n",
        "    publish_date: datetime.datetime\n",
        "    sentiment_score: float\n",
        "\n",
        "class AsyncPlanetscaleBatchFetcher:\n",
        "    def __init__(self, ps_conn_details, min_size=1, max_size=10):\n",
        "        self.ps_conn_details = ps_conn_details\n",
        "        self.min_size = min_size\n",
        "        self.max_size = max_size\n",
        "        self.pool = None\n",
        "\n",
        "    @classmethod\n",
        "    async def create(cls, ps_conn_details, min_size=1, max_size=10):\n",
        "        instance = cls(ps_conn_details, min_size, max_size)\n",
        "        await instance.create_pool()\n",
        "        return instance\n",
        "\n",
        "    async def create_pool(self):\n",
        "        if self.pool is None:\n",
        "            ca_path = '/etc/ssl/certs/ca-certificates.crt'\n",
        "            ssl_context = ssl.create_default_context(cafile=ca_path)  # Adjust the path to your CA certificate\n",
        "            self.pool = await aiomysql.create_pool(\n",
        "                user=self.ps_conn_details['user'],\n",
        "                password=self.ps_conn_details['password'],\n",
        "                host=self.ps_conn_details['host'],\n",
        "                db=self.ps_conn_details['database'],\n",
        "                minsize=self.min_size,\n",
        "                maxsize=self.max_size,\n",
        "                ssl=ssl_context\n",
        "            )\n",
        "\n",
        "    async def fetch_batches(self, table_name, skip_field=None, second_table=None, batch_size=500, max_rows=-1):\n",
        "        offset = 0\n",
        "        rows_fetched = 0\n",
        "\n",
        "        async with self.pool.acquire() as connection:\n",
        "            async with connection.cursor() as cursor:\n",
        "                # Execute the query once to get the column names\n",
        "                sample_query = f\"SELECT * FROM {table_name} LIMIT 1\"\n",
        "                await cursor.execute(sample_query)\n",
        "                sample_batch = await cursor.fetchone()\n",
        "                column_names = [desc[0] for desc in cursor.description] if sample_batch else []\n",
        "\n",
        "                while True:\n",
        "                    # Base query\n",
        "                    batch_query = f\"SELECT * FROM {table_name}\"\n",
        "\n",
        "                    # Add filtering if skip_field and second_table are provided\n",
        "                    if skip_field and second_table:\n",
        "                        batch_query += f\" WHERE {skip_field} NOT IN (SELECT {skip_field} FROM {second_table})\"\n",
        "\n",
        "                    # Add LIMIT and OFFSET\n",
        "                    batch_query += f\" LIMIT {batch_size} OFFSET {offset}\"\n",
        "\n",
        "                    # Fetch the batch\n",
        "                    await cursor.execute(batch_query)\n",
        "                    batch = await cursor.fetchall()\n",
        "\n",
        "                    # Break if no more rows or reached max_rows\n",
        "                    if not batch or (max_rows != -1 and rows_fetched >= max_rows):\n",
        "                        break\n",
        "\n",
        "                    # Convert the batch to a pandas DataFrame with the column names\n",
        "                    batch_df = pd.DataFrame(batch, columns=column_names)\n",
        "\n",
        "                    yield batch_df\n",
        "\n",
        "                    rows_fetched += len(batch)\n",
        "                    offset += batch_size\n",
        "\n",
        "                    # Break if reached max_rows\n",
        "                    if max_rows != -1 and rows_fetched >= max_rows:\n",
        "                        break\n",
        "\n",
        "    async def upload_sentiment_scores_to_db(self, sentiment_scores: List[SentimentScore]):\n",
        "        async with self.pool.acquire() as connection:\n",
        "            async with connection.cursor() as cursor:\n",
        "                insert_query = \"\"\"INSERT INTO sentiment_scores (article_id, stock_symbol, publish_date, sentiment_score)\n",
        "                                VALUES (%s, %s, %s, %s)\n",
        "                                ON DUPLICATE KEY UPDATE sentiment_score = VALUES(sentiment_score);\"\"\"\n",
        "                values = [(str(score.article_id), score.stock_symbol, score.publish_date, score.sentiment_score) for score in sentiment_scores]\n",
        "                await cursor.executemany(insert_query, values)\n",
        "                await connection.commit()\n",
        "\n",
        "\n",
        "    async def average_sentiment_scores(self):\n",
        "        async with self.pool.acquire() as connection:\n",
        "            async with connection.cursor() as cursor:\n",
        "                query = \"\"\"SELECT stock_symbol, publish_date, AVG(sentiment_score) AS avg_sentiment_score\n",
        "                    FROM sentiment_scores\n",
        "                    GROUP BY stock_symbol, publish_date;\n",
        "                    \"\"\"\n",
        "                await cursor.execute(query)\n",
        "                await connection.commit()\n",
        "        print(\"Averaged all sentiment scores per stock symbol per date.\")\n",
        "\n",
        "async def sentiment_to_score(sentiment_result: dict) -> float | None:\n",
        "    # print(sentiment_result)\n",
        "    label = sentiment_result.get(\"label\")\n",
        "    sentiment_sign: int = int(-1)\n",
        "    sentiment_score = sentiment_result.get(\"score\")\n",
        "    if type(sentiment_score) is not float:\n",
        "        print(f\"Sentiment score is not float.\\n  {sentiment_result}\")\n",
        "        return None\n",
        "    match label:\n",
        "        case 'positive':\n",
        "            sentiment_sign: int = int(1)\n",
        "            sentiment_score = sentiment_score * sentiment_sign\n",
        "        case 'neutral':\n",
        "            # Use 0 as a padding value in ML model, set to small positive to differentiate between padding and neutral\n",
        "            sentiment_sign: int = int(1)\n",
        "            sentiment_score = 0.0001\n",
        "        case'negative':\n",
        "            sentiment_sign: int = int(-1)\n",
        "            sentiment_score = sentiment_score * sentiment_sign\n",
        "        case _:\n",
        "            raise ValueError(f\"Sentiment label is not 'positive'/'neutral'/'negative', label: {label}\")\n",
        "\n",
        "    return sentiment_score\n",
        "\n",
        "async def assemble_SentimentScore(sentiment_result: dict | None, analysis_piece: dict) -> SentimentScore | None:\n",
        "    if sentiment_result is None:\n",
        "        return None\n",
        "    sentiment_score = await sentiment_to_score(sentiment_result)\n",
        "    if sentiment_score is not None:\n",
        "        analysis_piece['sentiment_score'] = sentiment_score\n",
        "        return SentimentScore(**analysis_piece)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    pipe = pipeline(\"text-classification\", model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
        "\n",
        "    # Load db credentials\n",
        "    with open(\"/content/drive/MyDrive/Machine_Learning_Digestor/config/ps_conn_details.json\", \"r\") as f:\n",
        "        ps_conn_details = json.load(f)\n",
        "\n",
        "    impact_score_db = await AsyncPlanetscaleBatchFetcher.create(ps_conn_details)\n",
        "    impact_score_db = fetcher\n",
        "    async for article_data in impact_score_db.fetch_batches(table_name='articles', batch_size=500, max_rows=-1, skip_field=\"article_id\", second_table=\"sentiment_scores\"):\n",
        "        print(article_data)\n",
        "        analysis_parts_generator = (\n",
        "        {\n",
        "            \"article_id\": row['article_id'],\n",
        "            \"stock_symbol\": row['stock_symbol'],\n",
        "            \"publish_date\": row['publish_date']\n",
        "        }\n",
        "        for _, row in article_data.iterrows())\n",
        "        summaries = article_data['summary']\n",
        "        tasks = []\n",
        "        sentiment_results = []\n",
        "        for summary in list(summaries):\n",
        "            try:\n",
        "                output = pipe(summary)\n",
        "                sentiment_results.append(output[0])\n",
        "            except:\n",
        "                sentiment_results.append(None)\n",
        "                continue\n",
        "\n",
        "        upload_tasks = []\n",
        "        for sentiment_result, analysis_piece in tqdm(zip(sentiment_results, analysis_parts_generator), total=len(article_data)):\n",
        "            task = asyncio.create_task(assemble_SentimentScore(sentiment_result=sentiment_result, analysis_piece=analysis_piece))\n",
        "            tasks.append(task)\n",
        "            sentiment_scores = [*filter(None, await asyncio.gather(*tasks))]\n",
        "\n",
        "            upload_task = asyncio.create_task(impact_score_db.upload_sentiment_scores_to_db(sentiment_scores))\n",
        "            upload_tasks.append(upload_task)\n",
        "        await asyncio.gather(*upload_tasks)"
      ],
      "metadata": {
        "id": "QhsmVKrX8dlE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}