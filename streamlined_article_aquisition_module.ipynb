{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOo5AmbLA/KxK65eXFPkoAq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/klutzydrummer/Python_Projects/blob/main/streamlined_article_aquisition_module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "pDJ3Xg_MF8vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Module Docstring\n",
        "\"\"\"\n",
        "import os\n",
        "import shutil\n",
        "import pip\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"Setuptools is replacing distutils.\")\n",
        "\n",
        "pyrequirements_path = Path(\"requirements.txt\")\n",
        "\n",
        "if not pyrequirements_path.exists():\n",
        "    with open(pyrequirements_path, \"w\") as project_file:\n",
        "        project_file.write('''numpy\n",
        "nltk\n",
        "aiomysql\n",
        "pyyaml >= 6.0.1\n",
        "tqdm >= 4.66.0\n",
        "googlenews >= 1.6.8\n",
        "newspaper3k >= 0.2.8\n",
        "asyncio >= 3.4.3\n",
        "pendulum >= 2.1.2\n",
        "psycopg2-binary >= 2.9.7''')\n",
        "\n",
        "    pip.main([\"install\", \"-r\", str(pyrequirements_path)])\n",
        "\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import dataclasses\n",
        "import datetime\n",
        "import functools\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import string\n",
        "import sys\n",
        "import traceback\n",
        "import uuid\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from pathlib import Path as path\n",
        "from typing import Callable, List, Optional, Generator, Awaitable\n",
        "\n",
        "import joblib\n",
        "import nltk\n",
        "from GoogleNews import GoogleNews\n",
        "from newspaper import Article, Source, Config\n",
        "import pandas as pd\n",
        "import pendulum\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "from tqdm.autonotebook import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pymysql\n",
        "\n",
        "import yfinance\n",
        "import random\n",
        "import pandas_datareader as pdr\n",
        "\n",
        "\n",
        "from dataclasses import asdict, fields, make_dataclass\n",
        "from typing import Type\n",
        "\n",
        "config_base = \"/content/drive/MyDrive/Machine_Learning_Digestor/config\" if path(\"/content/drive\").exists() else \"/content\"\n",
        "\n",
        "\n",
        "def generate_random_dates(start_date, end_date, num_dates):\n",
        "    # Convert the date objects to datetime if they are provided as strings\n",
        "    if isinstance(start_date, str):\n",
        "        start_date = datetime.datetime.strptime(start_date, \"%m/%d/%Y\")\n",
        "    if isinstance(end_date, str):\n",
        "        end_date = datetime.datetime.strptime(end_date, \"%m/%d/%Y\")\n",
        "\n",
        "    # Compute the time difference in days\n",
        "    delta = end_date - start_date\n",
        "\n",
        "    # If num_dates is equal to the number of available days, return all dates\n",
        "    if num_dates == delta.days + 1:\n",
        "        generated_dates = [start_date + datetime.timedelta(days=i) for i in range(delta.days + 1)]\n",
        "        random.shuffle(generated_dates)\n",
        "        return generated_dates\n",
        "    elif num_dates == 0:\n",
        "        generated_dates = [start_date + datetime.timedelta(days=i) for i in range(delta.days + 1)]\n",
        "        random.shuffle(generated_dates)\n",
        "        return generated_dates\n",
        "    # If num_dates is greater than available days, raise an error\n",
        "    if num_dates > delta.days + 1:\n",
        "        raise ValueError(\"num_dates is greater than the number of available days\")\n",
        "\n",
        "    random_date = None\n",
        "    generated_dates = []\n",
        "\n",
        "    while len(generated_dates) < num_dates:\n",
        "        # Generate a random number of days to add to the start date\n",
        "        random_days = random.randint(0, delta.days)\n",
        "\n",
        "        # Compute the random date\n",
        "        random_date = start_date + datetime.timedelta(days=random_days)\n",
        "\n",
        "        # Check if this date has been generated before\n",
        "        if random_date not in generated_dates:\n",
        "            generated_dates.append(random_date)\n",
        "\n",
        "    # Return the list of generated dates\n",
        "    return generated_dates\n",
        "\n",
        "def verbprint(*args, verbose=False, **kwargs):\n",
        "    if verbose is True:\n",
        "        print(*args, **kwargs)\n",
        "\n",
        "def ignore_first_arg(func: Callable[[str], str], _: object, arg: str) -> str:\n",
        "    return func(arg)\n",
        "\n",
        "def preprocess(text: str) -> str:\n",
        "    \"\"\"Preprocess the text.\"\"\"\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lowercasing\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "\n",
        "def custom_time_to_str(datetime: datetime.datetime) -> str:\n",
        "    return f\"{int(datetime.month):02}/{int(datetime.day):02}/{int(datetime.year)}\"\n",
        "\n",
        "def chunks(lst, n) -> Generator:\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i:i + n]\n",
        "\n",
        "\n",
        "@dataclasses.dataclass(init=False, repr=True, eq=True)\n",
        "class PreppedArticles:\n",
        "    title: str\n",
        "    text: str\n",
        "    summary: str\n",
        "    authors: list[str]\n",
        "    publish_date: datetime.datetime\n",
        "    search_date: datetime.datetime\n",
        "    tags: set[str]\n",
        "    keywords: set[str]\n",
        "    stock_symbol: str\n",
        "    search_string: str\n",
        "    url: str\n",
        "    article_id: uuid.UUID\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_non_ascii(a_str):\n",
        "        ascii_chars = string.printable\n",
        "\n",
        "        return ''.join(\n",
        "            filter(lambda x: x in ascii_chars, a_str)\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def str_to_uuid(cls, in_string: str) -> uuid.UUID:\n",
        "        hash_machine = hashlib.md5()\n",
        "        filtered_string = cls.remove_non_ascii(in_string)\n",
        "        hash_machine.update(bytes(filtered_string, \"ascii\"))\n",
        "        hash_bytes = int(hash_machine.hexdigest(), 16)\n",
        "        return uuid.UUID(int=hash_bytes)\n",
        "\n",
        "    @classmethod\n",
        "    def custom_uuid(cls, stock_symbol, title):\n",
        "        return cls.str_to_uuid(f\"{stock_symbol} {title}\")\n",
        "\n",
        "    def __init__(self, title: str, text: str, summary: str, authors: list[str], publish_date: datetime.datetime, search_date: datetime.datetime, tags: set[str], keywords: set[str], stock_symbol: str, search_string: str, url: str):\n",
        "        self.title = title\n",
        "        self.text = text\n",
        "        self.summary = summary\n",
        "        self.authors = authors\n",
        "        self.publish_date = publish_date\n",
        "        self.search_date = search_date\n",
        "        self.tags = tags\n",
        "        self.keywords = keywords\n",
        "        self.stock_symbol = stock_symbol\n",
        "        self.search_string = search_string\n",
        "        self.article_id = self.custom_uuid(stock_symbol=self.stock_symbol,title=self.title)\n",
        "        self.url = url\n",
        "\n",
        "    @classmethod\n",
        "    def import_newspaper(cls, newspaper_article: Article, stock_symbol: str, search_string: str, search_date: datetime.datetime) -> 'cls':\n",
        "        if type(newspaper_article.publish_date) is not datetime.datetime:\n",
        "            raise ValueError(f\"Publish date is not of type datetime:\\n type:\\n {type(newspaper_article.publish_date)}\\n value:\\n {newspaper_article.publish_date}\")\n",
        "        result = cls(\n",
        "            title=newspaper_article.title,\n",
        "            text=newspaper_article.text,\n",
        "            summary=newspaper_article.summary,\n",
        "            authors=newspaper_article.authors,\n",
        "            publish_date=datetime.datetime.fromtimestamp(newspaper_article.publish_date.timestamp()),\n",
        "            search_date=search_date,\n",
        "            tags=set(newspaper_article.tags),\n",
        "            keywords=set(newspaper_article.keywords),\n",
        "            stock_symbol=stock_symbol,\n",
        "            search_string=search_string,\n",
        "            url=newspaper_article.canonical_link\n",
        "        )\n",
        "        return result\n",
        "\n",
        "    def astuple(self) -> tuple:\n",
        "        return dataclasses.astuple(self)\n",
        "\n",
        "    def asdict(self) -> dict:\n",
        "        return dataclasses.asdict(self)\n",
        "\n",
        "\n",
        "def search_google(search_string: str) -> list[str]:\n",
        "    googlenews = GoogleNews(lang=\"en\", encode=\"utf-8\")\n",
        "    googlenews.clear()\n",
        "    googlenews.enableException(True)\n",
        "    googlenews.get_news(search_string)\n",
        "    return googlenews.get_links()\n",
        "\n",
        "\n",
        "\n",
        "class DatabaseHandler:\n",
        "    def __init__(self, conn_details):\n",
        "        self.conn_details = conn_details\n",
        "        self.loop = asyncio.get_event_loop()\n",
        "        self.pool = ThreadPoolExecutor(max_workers=5)  # Adjust max_workers based on your needs\n",
        "        self._dataclass_cache = {}\n",
        "\n",
        "    def connect(self):\n",
        "        return pymysql.connect(\n",
        "            user=self.conn_details['user'],\n",
        "            password=self.conn_details['password'],\n",
        "            host=self.conn_details['host'],\n",
        "            database=self.conn_details['database'],\n",
        "            ssl={'require': True}\n",
        "        )\n",
        "\n",
        "    def upload_prepped_articles(self, articles: List[PreppedArticles]):\n",
        "        connection = self.connect()\n",
        "        cursor = connection.cursor()\n",
        "\n",
        "        query = \"\"\"\n",
        "        INSERT IGNORE INTO articles (\n",
        "            article_id, authors, keywords, publish_date, search_date,\n",
        "            search_string, stock_symbol, summary, tags, text, title, url\n",
        "        ) VALUES (\n",
        "            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s\n",
        "        );\n",
        "        \"\"\"\n",
        "\n",
        "        data_to_insert = [\n",
        "            (\n",
        "                str(article.article_id), json.dumps(article.authors), json.dumps(list(article.keywords)),\n",
        "                article.publish_date, article.search_date, article.search_string,\n",
        "                article.stock_symbol, article.summary, json.dumps(list(article.tags)),\n",
        "                article.text, article.title, article.url\n",
        "            )\n",
        "            for article in articles\n",
        "        ]\n",
        "\n",
        "        cursor.executemany(query, data_to_insert)\n",
        "\n",
        "        connection.commit()\n",
        "        cursor.close()\n",
        "        connection.close()\n",
        "\n",
        "\n",
        "\n",
        "def share_elements(set1, set2, n):\n",
        "    set1_ready = set([str(item).lower() for item in set1])\n",
        "    set2_ready = set([str(item).lower() for item in set2])\n",
        "    common_elements = len(set(set1_ready).intersection(set(set2_ready)))\n",
        "    return common_elements >= n\n",
        "\n",
        "async def validate_article(article: Article, mandatory_keywords: list[str], total_match_keywords: int, verbose=False) -> Article | None:\n",
        "    try:\n",
        "        if type(article.publish_date) is not datetime.datetime:\n",
        "            raise ValueError(\"Publish_date not of type datetime.\")\n",
        "        if \"Are you a robot?\" in article.title or \"403 Client Error\" in article.text:\n",
        "            raise ValueError(\"Crawling not permitted.\")\n",
        "        if article.text == \"\":\n",
        "            raise ValueError(\"Article text empty.\")\n",
        "        if share_elements(article.keywords, mandatory_keywords, total_match_keywords) is not True:\n",
        "            raise ValueError(\"Article did not match enough mandatory keywords.\")\n",
        "        return article\n",
        "    except Exception as err:\n",
        "        if verbose is True:\n",
        "            print(err)\n",
        "        return None\n",
        "\n",
        "def prereq():\n",
        "    if path('/root/nltk_data').exists() is not True:\n",
        "        nltk.download('stopwords')\n",
        "        nltk.download('wordnet')\n",
        "        nltk.download('punkt')\n",
        "\n",
        "async def pipeline(stock_symbol: str, search_date_start: datetime.datetime, search_date_end: datetime.datetime, num_dates: int, postgres_connection_string: str, mandatory_keywords: list[str], total_match_keywords: int, logfile: path | None = None, loglevel: str = \"\", verbose: bool = False, dry_run: bool = False):\n",
        "    \"\"\"\n",
        "    Asynchronous pipeline function to process stock-related data and upload the processed articles to a postgres database.\n",
        "\n",
        "    :param stock_symbol: The stock symbol representing a specific stock, given as a string.\n",
        "    :param search_date: The date for which to search stock-related data, given as a datetime object.\n",
        "    :param llama_instance: An instance of the LlamaCpp class responsible for processing or analyzing data.\n",
        "    :param embeddings_instance: An instance of the HuggingFaceInstructEmbeddings class for generating or managing embeddings.\n",
        "    :param postgres_connection_string: The connection string for connecting to a PostgreSQL database, given as a string.\n",
        "    :param logfile: Optional path to a file where log information will be stored. Defaults to None, indicating no logging to a file.\n",
        "    :param loglevel: Optional string specifying the logging level, such as \"INFO\", \"WARNING\", \"ERROR\", etc. Defaults to an empty string, indicating default logging behavior.\n",
        "\n",
        "    :return: None.\n",
        "    \"\"\"\n",
        "\n",
        "    if logfile is None:\n",
        "        logfile = path('/content/article_aquisition_log.log')\n",
        "    logfile = path(logfile)\n",
        "    logfile.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    log_message = f\"Loglevel set to {loglevel}\"\n",
        "    match loglevel:\n",
        "        case 'debug':\n",
        "            loglevel_type = logging.DEBUG\n",
        "        case 'info':\n",
        "            loglevel_type = logging.INFO\n",
        "        case 'warn':\n",
        "            log_message = \"logging.WARN is deprecated, setting Logging level to logging.WARNING.\"\n",
        "            loglevel_type = logging.WARNING\n",
        "        case 'warning':\n",
        "            loglevel_type = logging.WARNING\n",
        "        case 'error':\n",
        "            loglevel_type = logging.ERROR\n",
        "        case 'critical':\n",
        "            loglevel_type = logging.CRITICAL\n",
        "        case 'fatal':\n",
        "            loglevel_type = logging.FATAL\n",
        "        case _:\n",
        "            loglevel_type = logging.DEBUG\n",
        "            log_message = f\"You have typed an invalid loglevel: {loglevel}\\n    Logging level set to DEBUG.\"\n",
        "    if verbose is True:\n",
        "        loglevel_type = logging.DEBUG\n",
        "        log_message = \"LLM verbosity set to True.\\n    Logging level set to debug.\"\n",
        "\n",
        "    logging.debug(\"logging level set\")\n",
        "    logging.basicConfig(filename=str(logfile), encoding='utf-8', level=loglevel_type)\n",
        "    logging.getLogger().setLevel(loglevel_type)\n",
        "    logging.info(log_message)\n",
        "\n",
        "    config = Config()\n",
        "    config.MAX_THREADS_PER_SOURCE = 10\n",
        "    google_news_url = 'https://news.google.com'\n",
        "\n",
        "    news_list = []\n",
        "    generated_dates = generate_random_dates(search_date_start, search_date_end, num_dates)\n",
        "    for search_date_obj in generated_dates:\n",
        "        search_date_str = custom_time_to_str(search_date_obj)\n",
        "        search_string = f\"stock news {stock_symbol} {search_date_str}\"\n",
        "        sub_news_list = search_google(search_string)\n",
        "        news_list.extend(sub_news_list)\n",
        "        logging.debug(f\"Search created for: {search_date_str}\")\n",
        "        logging.debug(\"All searches completed.\")\n",
        "\n",
        "\n",
        "\n",
        "        source = Source(google_news_url, config=config)\n",
        "        logging.debug(f\"Aquired {len(sub_news_list)} urls.\")\n",
        "\n",
        "        for url in sub_news_list:\n",
        "            source.articles.append(Article(f\"https://{url}\"))\n",
        "\n",
        "        source.download_articles()\n",
        "        try:\n",
        "            source.parse_articles()\n",
        "            logging.debug(\"Bulk parse completed\")\n",
        "        except:\n",
        "            for article in source.articles:\n",
        "                try:\n",
        "                    article.parse()\n",
        "                except:\n",
        "                    continue\n",
        "            logging.debug(\"Indvidual articles parsed due to error in bulk parsing.\")\n",
        "        tasks = []\n",
        "        for article in source.articles:\n",
        "            if article.is_parsed is True:\n",
        "                article.nlp()\n",
        "                tasks.append(asyncio.create_task(validate_article(article=article, mandatory_keywords=mandatory_keywords, total_match_keywords=total_match_keywords)))\n",
        "        logging.debug(\"Performed nlp on all articles.\")\n",
        "        valid_articles = [*filter(None, await asyncio.gather(*tasks))]\n",
        "        logging.debug(f\"Validated {len(valid_articles)} urls.\")\n",
        "        prepped_articles = [PreppedArticles.import_newspaper(newspaper_article=article, stock_symbol=stock_symbol, search_string=search_string, search_date=search_date_obj) for article in valid_articles]\n",
        "        logging.debug(\"All valid articles prepared for upload to db.\")\n",
        "\n",
        "\n",
        "        db_handler = DatabaseHandler(ps_conn_details)\n",
        "\n",
        "\n",
        "        try:\n",
        "            db_handler.upload_prepped_articles(prepped_articles)\n",
        "            logging.debug(\"Uploaded articles.\")\n",
        "        except Exception as err:\n",
        "            logging.debug(\"Encountered Error. I'm not usre these errors are being logged by logging.ERROR\")\n",
        "            logging.error(err)\n",
        "            pass\n",
        "    logging.info(\"Processing pipeline completed.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    stock_symbol = \"MSFT\"\n",
        "    # start_date = datetime.datetime(day=1, month=1, year=2022)\n",
        "    end_date = datetime.datetime.now()\n",
        "    start_date = end_date -  datetime.timedelta(days=5)\n",
        "    end_date = '5/24/2020'\n",
        "    start_date = '1/2/1962'\n",
        "    # search_date = \"08/13/2023\"\n",
        "    mandatory_keywords = [\n",
        "        \"Stock\",\n",
        "        \"Stocks\",\n",
        "        \"Stock Market\",\n",
        "        \"Shares\",\n",
        "        \"Dividend\",\n",
        "        \"Portfolio\",\n",
        "        \"Investment\",\n",
        "        \"Trading\",\n",
        "        \"Exchange\",\n",
        "        \"Securities\",\n",
        "        \"Bull Market\",\n",
        "        \"Bear Market\",\n",
        "        \"IPO\",\n",
        "        \"Equity\",\n",
        "        \"Bonds\",\n",
        "        \"Index\",\n",
        "        \"Financial Market\"\n",
        "    ]\n",
        "    total_match_keywords = 2\n",
        "\n",
        "    # search_date_obj = datetime.datetime.strptime(search_date, \"%m/%d/%Y\")\n",
        "\n",
        "    logfile = path('/content/article_aquisition_log.log')\n",
        "    loglevel = 'info'\n",
        "\n",
        "    with open(f\"{config_base}/connection_string.json\", \"r\") as f:\n",
        "        postgres_connection_string = json.load(f)\n",
        "\n",
        "    with open(f\"{config_base}/ps_conn_details_preprocesser.json\", \"r\") as f:\n",
        "        ps_conn_details = json.load(f)\n",
        "        del ps_conn_details[\"ssl\"]\n",
        "\n",
        "    # loglevel = 'debug'\n",
        "\n",
        "    #stock_symbol_list = [\"MMM\",\"AOS\",\"ABT\",\"ABBV\",\"ACN\",\"ATVI\",\"ADM\",\"ADBE\",\"ADP\",\"AAP\",\"AES\",\"AFL\",\"A\",\"APD\",\"AKAM\",\"ALK\",\"ALB\",\"ARE\",\"ALGN\",\"ALLE\",\"LNT\",\"ALL\",\"GOOGL\",\"GOOG\",\"MO\",\"AMZN\",\"AMCR\",\"AMD\",\"AEE\",\"AAL\",\"AEP\",\"AXP\",\"AIG\",\"AMT\",\"AWK\",\"AMP\",\"ABC\",\"AME\",\"AMGN\",\"APH\",\"ADI\",\"ANSS\",\"AON\",\"APA\",\"AAPL\",\"AMAT\",\"APTV\",\"ACGL\",\"ANET\",\"AJG\",\"AIZ\",\"T\",\"ATO\",\"ADSK\",\"AZO\",\"AVB\",\"AVY\",\"AXON\",\"BKR\",\"BALL\",\"BAC\",\"BBWI\",\"BAX\",\"BDX\",\"WRB\",\"BRK.B\",\"BBY\",\"BIO\",\"TECH\",\"BIIB\",\"BLK\",\"BK\",\"BA\",\"BKNG\",\"BWA\",\"BXP\",\"BSX\",\"BMY\",\"AVGO\",\"BR\",\"BRO\",\"BF.B\",\"BG\",\"CHRW\",\"CDNS\",\"CZR\",\"CPT\",\"CPB\",\"COF\",\"CAH\",\"KMX\",\"CCL\",\"CARR\",\"CTLT\",\"CAT\",\"CBOE\",\"CBRE\",\"CDW\",\"CE\",\"CNC\",\"CNP\",\"CDAY\",\"CF\",\"CRL\",\"SCHW\",\"CHTR\",\"CVX\",\"CMG\",\"CB\",\"CHD\",\"CI\",\"CINF\",\"CTAS\",\"CSCO\",\"C\",\"CFG\",\"CLX\",\"CME\",\"CMS\",\"KO\",\"CTSH\",\"CL\",\"CMCSA\",\"CMA\",\"CAG\",\"COP\",\"ED\",\"STZ\",\"CEG\",\"COO\",\"CPRT\",\"GLW\",\"CTVA\",\"CSGP\",\"COST\",\"CTRA\",\"CCI\",\"CSX\",\"CMI\",\"CVS\",\"DHI\",\"DHR\",\"DRI\",\"DVA\",\"DE\",\"DAL\",\"XRAY\",\"DVN\",\"DXCM\",\"FANG\",\"DLR\",\"DFS\",\"DIS\",\"DG\",\"DLTR\",\"D\",\"DPZ\",\"DOV\",\"DOW\",\"DTE\",\"DUK\",\"DD\",\"DXC\",\"EMN\",\"ETN\",\"EBAY\",\"ECL\",\"EIX\",\"EW\",\"EA\",\"ELV\",\"LLY\",\"EMR\",\"ENPH\",\"ETR\",\"EOG\",\"EPAM\",\"EQT\",\"EFX\",\"EQIX\",\"EQR\",\"ESS\",\"EL\",\"ETSY\",\"EG\",\"EVRG\",\"ES\",\"EXC\",\"EXPE\",\"EXPD\",\"EXR\",\"XOM\",\"FFIV\",\"FDS\",\"FICO\",\"FAST\",\"FRT\",\"FDX\",\"FITB\",\"FSLR\",\"FE\",\"FIS\",\"FI\",\"FLT\",\"FMC\",\"F\",\"FTNT\",\"FTV\",\"FOXA\",\"FOX\",\"BEN\",\"FCX\",\"GRMN\",\"IT\",\"GEHC\",\"GEN\",\"GNRC\",\"GD\",\"GE\",\"GIS\",\"GM\",\"GPC\",\"GILD\",\"GL\",\"GPN\",\"GS\",\"HAL\",\"HIG\",\"HAS\",\"HCA\",\"PEAK\",\"HSIC\",\"HSY\",\"HES\",\"HPE\",\"HLT\",\"HOLX\",\"HD\",\"HON\",\"HRL\",\"HST\",\"HWM\",\"HPQ\",\"HUM\",\"HBAN\",\"HII\",\"IBM\",\"IEX\",\"IDXX\",\"ITW\",\"ILMN\",\"INCY\",\"IR\",\"PODD\",\"INTC\",\"ICE\",\"IFF\",\"IP\",\"IPG\",\"INTU\",\"ISRG\",\"IVZ\",\"INVH\",\"IQV\",\"IRM\",\"JBHT\",\"JKHY\",\"J\",\"JNJ\",\"JCI\",\"JPM\",\"JNPR\",\"K\",\"KDP\",\"KEY\",\"KEYS\",\"KMB\",\"KIM\",\"KMI\",\"KLAC\",\"KHC\",\"KR\",\"LHX\",\"LH\",\"LRCX\",\"LW\",\"LVS\",\"LDOS\",\"LEN\",\"LNC\",\"LIN\",\"LYV\",\"LKQ\",\"LMT\",\"L\",\"LOW\",\"LYB\",\"MTB\",\"MRO\",\"MPC\",\"MKTX\",\"MAR\",\"MMC\",\"MLM\",\"MAS\",\"MA\",\"MTCH\",\"MKC\",\"MCD\",\"MCK\",\"MDT\",\"MRK\",\"META\",\"MET\",\"MTD\",\"MGM\",\"MCHP\",\"MU\",\"MSFT\",\"MAA\",\"MRNA\",\"MHK\",\"MOH\",\"TAP\",\"MDLZ\",\"MPWR\",\"MNST\",\"MCO\",\"MS\",\"MOS\",\"MSI\",\"MSCI\",\"NDAQ\",\"NTAP\",\"NFLX\",\"NWL\",\"NEM\",\"NWSA\",\"NWS\",\"NEE\",\"NKE\",\"NI\",\"NDSN\",\"NSC\",\"NTRS\",\"NOC\",\"NCLH\",\"NRG\",\"NUE\",\"NVDA\",\"NVR\",\"NXPI\",\"ORLY\",\"OXY\",\"ODFL\",\"OMC\",\"ON\",\"OKE\",\"ORCL\",\"OGN\",\"OTIS\",\"PCAR\",\"PKG\",\"PANW\",\"PARA\",\"PH\",\"PAYX\",\"PAYC\",\"PYPL\",\"PNR\",\"PEP\",\"PFE\",\"PCG\",\"PM\",\"PSX\",\"PNW\",\"PXD\",\"PNC\",\"POOL\",\"PPG\",\"PPL\",\"PFG\",\"PG\",\"PGR\",\"PLD\",\"PRU\",\"PEG\",\"PTC\",\"PSA\",\"PHM\",\"QRVO\",\"PWR\",\"QCOM\",\"DGX\",\"RL\",\"RJF\",\"RTX\",\"O\",\"REG\",\"REGN\",\"RF\",\"RSG\",\"RMD\",\"RVTY\",\"RHI\",\"ROK\",\"ROL\",\"ROP\",\"ROST\",\"RCL\",\"SPGI\",\"CRM\",\"SBAC\",\"SLB\",\"STX\",\"SEE\",\"SRE\",\"NOW\",\"SHW\",\"SPG\",\"SWKS\",\"SJM\",\"SNA\",\"SEDG\",\"SO\",\"LUV\",\"SWK\",\"SBUX\",\"STT\",\"STLD\",\"STE\",\"SYK\",\"SYF\",\"SNPS\",\"SYY\",\"TMUS\",\"TROW\",\"TTWO\",\"TPR\",\"TRGP\",\"TGT\",\"TEL\",\"TDY\",\"TFX\",\"TER\",\"TSLA\",\"TXN\",\"TXT\",\"TMO\",\"TJX\",\"TSCO\",\"TT\",\"TDG\",\"TRV\",\"TRMB\",\"TFC\",\"TYL\",\"TSN\",\"USB\",\"UDR\",\"ULTA\",\"UNP\",\"UAL\",\"UPS\",\"URI\",\"UNH\",\"UHS\",\"VLO\",\"VTR\",\"VRSN\",\"VRSK\",\"VZ\",\"VRTX\",\"VFC\",\"VTRS\",\"VICI\",\"V\",\"VMC\",\"WAB\",\"WBA\",\"WMT\",\"WBD\",\"WM\",\"WAT\",\"WEC\",\"WFC\",\"WELL\",\"WST\",\"WDC\",\"WRK\",\"WY\",\"WHR\",\"WMB\",\"WTW\",\"GWW\",\"WYNN\",\"XEL\",\"XYL\",\"YUM\",\"ZBRA\",\"ZBH\",\"ZION\",\"ZTS\"]\n",
        "    temp_stock_symbol_list = list(pdr.get_nasdaq_symbols().index)\n",
        "    # specific_targets = [\"IBM\"]\n",
        "    specific_targets = [\"MSFT\", \"AAPL\", \"TSLA\"]\n",
        "    specific_target_percentage = 10\n",
        "    percentage_multiplier = int(len(temp_stock_symbol_list) // 100 * specific_target_percentage)\n",
        "    specific_targets_multiplied = [item for item in specific_targets for _ in range(percentage_multiplier)]\n",
        "    temp_stock_symbol_list.extend(specific_targets_multiplied)\n",
        "    random.shuffle(temp_stock_symbol_list)\n",
        "    stock_symbol_list = specific_targets\n",
        "    stock_symbol_list.extend(temp_stock_symbol_list)\n",
        "    print(stock_symbol_list)\n",
        "    prereq()\n",
        "    while True:\n",
        "        for stock_symbol in tqdm(stock_symbol_list):\n",
        "            print(f\"stock_symbol: {stock_symbol}\")\n",
        "            await pipeline(\n",
        "                stock_symbol=stock_symbol,\n",
        "                search_date_start=start_date,\n",
        "                search_date_end=end_date,\n",
        "                num_dates=0,\n",
        "                postgres_connection_string=postgres_connection_string,\n",
        "                mandatory_keywords=mandatory_keywords,\n",
        "                total_match_keywords=total_match_keywords,\n",
        "                logfile=logfile,\n",
        "                loglevel=loglevel,\n",
        "                dry_run=False\n",
        "            )"
      ],
      "metadata": {
        "id": "EP8RdR3cl0Mc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}