{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyObT6KNd4s43n/zXao8bwGj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/klutzydrummer/Python-ML_Self_Learning/blob/main/Better_PaCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XZhZmcSNjgrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## My attempt at implementing and experimenting with the concepts present in the paper: Parsimonious Concept Engineering (PaCE)\n",
        "Github: [Link](https://github.com/peterljq/Parsimonious-Concept-Engineering)  \n",
        "Arxiv: [Link](https://arxiv.org/abs/2406.04331)  \n",
        "Project Page: [Link](https://peterljq.github.io/project/pace/index.html)  "
      ],
      "metadata": {
        "id": "0hJjr2Bnjvna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ColabBuddy v2.2\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "from pprint import pprint\n",
        "import re\n",
        "from typing import List, Dict, Tuple, Literal, Callable, Optional\n",
        "import requests\n",
        "from functools import lru_cache\n",
        "import subprocess\n",
        "import pkg_resources\n",
        "import packaging.version\n",
        "import importlib\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "class BaseBuddy:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def run_command(command: str, background: bool = False):\n",
        "        if isinstance(command, list):\n",
        "            command = ' '.join(command)\n",
        "        if background:\n",
        "            command = f'nohup bash -c \"{command}\"'\n",
        "        process = subprocess.Popen(\n",
        "            command,\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE,\n",
        "            shell=True,\n",
        "            universal_newlines=True,\n",
        "            bufsize=1  # Line-buffered mode for real-time output\n",
        "        )\n",
        "\n",
        "        while True:\n",
        "            output = process.stdout.readline()\n",
        "            error = process.stderr.readline()\n",
        "\n",
        "            if output:\n",
        "                print(output, end=\"\")\n",
        "                sys.stdout.flush()  # Flush the output immediately\n",
        "            if error:\n",
        "                print(error, end=\"\")\n",
        "                sys.stderr.flush()  # Flush the error immediately\n",
        "\n",
        "            # Break the loop if both output and error are empty and the process has ended\n",
        "            if not output and not error and process.poll() is not None:\n",
        "                break\n",
        "\n",
        "        return process.poll()\n",
        "\n",
        "class GitHubRepo:\n",
        "    def __init__(self, url: str, path: Path):\n",
        "        self.name = url.split('/')[-1].split('.')[0]\n",
        "        self.url = url\n",
        "        self.path = Path(path) / self.name\n",
        "\n",
        "    def clone(self):\n",
        "        if self.path.exists() is not True:\n",
        "            subprocess.run([\"git\", \"clone\", self.url, self.path])\n",
        "\n",
        "    def cd(self):\n",
        "        os.chdir(self.path)\n",
        "        print(f\"Changed directory to {self.path}\")\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        \"\"\"Dynamically create and return a method for the given GitHub command if not predefined.\"\"\"\n",
        "        if name in [\"name\", \"url\", \"path\", \"clone\", \"cd\"]:\n",
        "            return getattr(self, name)\n",
        "\n",
        "        premapped_flags = {\n",
        "            \"all\": \"-a\"\n",
        "        }\n",
        "        premapped_commands = {\n",
        "            \"commit\": \"commit -m\"\n",
        "        }\n",
        "        def dynamic_method(*args, **kwargs):\n",
        "            try:\n",
        "                # Attempt to run the command as a Git command\n",
        "                name_parts = name.split(\"_\")\n",
        "                name_parts[-1] = premapped_flags.get(name_parts[-1], name_parts[-1])\n",
        "                name_parts = [premapped_commands.get(cmd_piece, cmd_piece) for cmd_piece in name_parts]\n",
        "                command = ['git', *name_parts]\n",
        "                command_with_args = command + list(args)\n",
        "                result = subprocess.run(command_with_args, cwd=self.path, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "                # Optionally print the output of the command\n",
        "                print(result.stdout.decode())\n",
        "\n",
        "                # If the command was successful, create a static method\n",
        "                def successful_method(*args, **kwargs):\n",
        "                    command_with_args = command + list(args)\n",
        "                    return subprocess.run(command_with_args, cwd=self.path, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE).stdout.decode()\n",
        "\n",
        "                # Set the method as an attribute\n",
        "                setattr(self, name, successful_method)\n",
        "\n",
        "                return result.stdout.decode()\n",
        "\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                # Handle errors if the command fails\n",
        "                print(f\"Failed to run {name} with error: {e.stderr.decode()}\")\n",
        "                return None\n",
        "\n",
        "        # Return the dynamic method (callable)\n",
        "        return dynamic_method\n",
        "\n",
        "class RepoMan:\n",
        "    def __init__(self):\n",
        "        # Directly set the repos attribute without triggering __setattr__\n",
        "        self.repos = {}\n",
        "\n",
        "    def __setattr__(self, name, value):\n",
        "        if name == 'repos':\n",
        "            # Allow setting the repos attribute without type check\n",
        "            super().__setattr__(name, value)\n",
        "        elif isinstance(value, GitHubRepo):\n",
        "            self.repos[name] = value\n",
        "        else:\n",
        "            raise TypeError(f\"Value for {name} must be a GitHubRepo instance.\")\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        if name not in self.repos:\n",
        "            raise AttributeError(f\"No repo named {name} found.\")\n",
        "        return self.repos[name]\n",
        "\n",
        "    def __getitem__(self, name):\n",
        "        if name not in self.repos:\n",
        "            raise KeyError(f\"No repo named {name} found.\")\n",
        "        return self.repos[name]\n",
        "\n",
        "    def __setitem__(self, name, value):\n",
        "        if isinstance(value, GitHubRepo):\n",
        "            self.repos[name] = value\n",
        "        else:\n",
        "            raise TypeError(f\"Value for {name} must be a GitHubRepo instance.\")\n",
        "\n",
        "    def __contains__(self, name):\n",
        "        return name in self.repos\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.repos)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.repos)\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(json.dumps(self.repos))\n",
        "\n",
        "    def __repr__(self):\n",
        "        return repr(self.repos)\n",
        "\n",
        "    def __del__(self):\n",
        "        for repo in self.repos.values():\n",
        "            if repo is not None:\n",
        "                del repo\n",
        "\n",
        "    def __delattr__(self, name):\n",
        "        if name in self.repos:\n",
        "            del self.repos[name]\n",
        "        else:\n",
        "            raise AttributeError(f\"No repo named {name} found.\")\n",
        "\n",
        "    def __delitem__(self, name):\n",
        "        if name in self.repos:\n",
        "            del self.repos[name]\n",
        "        else:\n",
        "            raise KeyError(f\"No repo named {name} found.\")\n",
        "\n",
        "    def add_repo(self, repo: GitHubRepo):\n",
        "        self.repos[repo.name] = repo\n",
        "\n",
        "    def get(self, name: str, default=None):\n",
        "        return self.repos.get(name, default)\n",
        "\n",
        "class RepoBuddy:\n",
        "    def __init__(self):\n",
        "        self.repo_manager = RepoMan()\n",
        "\n",
        "    def git_clone(self, url, directory=None) -> GitHubRepo:\n",
        "        if directory is None:\n",
        "            directory = Path(os.getcwd())\n",
        "        else:\n",
        "            directory = Path(directory)\n",
        "        repo = GitHubRepo(url=url, path=directory)\n",
        "        try:\n",
        "            repo.clone()\n",
        "            print(\"Succesfully cloned repo: {repo}.\".format(repo=repo.name))\n",
        "            self.repo_manager.add_repo(repo)\n",
        "            return repo\n",
        "        except:\n",
        "            raise Exception(\"Failed to clone repo: {repo}.\".format(repo=repo.name))\n",
        "\n",
        "    def get_repo(self, name: str, default=None) -> Optional[GitHubRepo]:\n",
        "        repo = self.repo_manager.get(name, default)\n",
        "        return repo\n",
        "\n",
        "\n",
        "class PacManBuddy(BaseBuddy):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @classmethod\n",
        "    def parse_version_safe(cls, version):\n",
        "        \"\"\"\n",
        "        Safely parse a version string, handling wildcard versions by returning\n",
        "        the prefix as a parsed version or None if the version is invalid.\n",
        "        \"\"\"\n",
        "        wildcard_flag = False\n",
        "        if '*' in version:\n",
        "            # Strip wildcard and parse the remaining part\n",
        "            version = version.split('*')[0].rstrip('.')\n",
        "            wildcard_flag = True\n",
        "        try:\n",
        "            return packaging.version.parse(version), wildcard_flag\n",
        "        except packaging.version.InvalidVersion:\n",
        "            return None, wildcard_flag\n",
        "\n",
        "    @classmethod\n",
        "    def compare_versions(cls, current_version, target_version, operator):\n",
        "        \"\"\"\n",
        "        Compare two versions with a given operator, handling wildcard scenarios.\n",
        "        \"\"\"\n",
        "        if operator == \"==\":\n",
        "            return current_version == target_version\n",
        "        elif operator == \"!=\":\n",
        "            return current_version != target_version\n",
        "        elif operator == \"<\":\n",
        "            return current_version < target_version\n",
        "        elif operator == \"<=\":\n",
        "            return current_version <= target_version\n",
        "        elif operator == \">\":\n",
        "            return current_version > target_version\n",
        "        elif operator == \">=\":\n",
        "            return current_version >= target_version\n",
        "        elif operator == \"~=\":\n",
        "            # Handle the compatible release clause\n",
        "            return (current_version.major == target_version.major and\n",
        "                    current_version.minor == target_version.minor and\n",
        "                    current_version >= target_version)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported version operator: {operator}\")\n",
        "\n",
        "class PipBuddy(PacManBuddy):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.available_pip_pkg_versions = {}\n",
        "\n",
        "    @property\n",
        "    def pip_pkg_versions(self):\n",
        "        return {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
        "\n",
        "    def get_pip_pkg_version(self, pkg):\n",
        "        return self.pip_pkg_versions.get(pkg, None)\n",
        "\n",
        "    @staticmethod\n",
        "    @lru_cache(maxsize=128)\n",
        "    def _get_available_pip_package_versions(package):\n",
        "        # Query the PyPI JSON API for the package\n",
        "        response = requests.get(f\"https://pypi.org/pypi/{package}/json\")\n",
        "\n",
        "        # Check if the request was successful\n",
        "        if response.status_code != 200:\n",
        "            raise ValueError(f\"Error fetching package data: {response.status_code}\")\n",
        "\n",
        "        # Parse the JSON data\n",
        "        package_data = response.json()\n",
        "\n",
        "        # Extract all available versions\n",
        "        available_versions = package_data[\"releases\"].keys()\n",
        "\n",
        "        # Sort the versions using packaging.version.parse for correct ordering\n",
        "        available_versions = sorted(available_versions, key=packaging.version.parse, reverse=True)\n",
        "\n",
        "        return available_versions\n",
        "\n",
        "    def get_available_pip_package_versions(self, package):\n",
        "        if package not in self.available_pip_pkg_versions:\n",
        "            self.available_pip_pkg_versions[package] = self._get_available_pip_package_versions(package)\n",
        "        return self.available_pip_pkg_versions[package]\n",
        "\n",
        "    def get_latest_pip_package_versions(self, package):\n",
        "        if package not in self.available_pip_pkg_versions:\n",
        "            self.available_pip_pkg_versions[package] = self._get_available_pip_package_versions(package)\n",
        "        return self.available_pip_pkg_versions[package][0]\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_pip_package_string(command: str) -> Dict[str, Dict[str, Optional[str]]]:\n",
        "        \"\"\"\n",
        "        Parse a pip install command to extract package names and their version specifications.\n",
        "\n",
        "        Args:\n",
        "            command (str): The pip install command string.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary where the key is the package name, and the value is a dictionary\n",
        "                with 'version_operator' and 'version' keys. If the version is not specified,\n",
        "                they will be None.\n",
        "        \"\"\"\n",
        "        # Ensure the command starts with 'pip install'\n",
        "        if not command.startswith('pip install'):\n",
        "            raise ValueError(\"The command must start with 'pip install'\")\n",
        "\n",
        "        # Remove 'pip install' and strip any extra spaces\n",
        "        package_string = command.replace('pip install', '').strip()\n",
        "\n",
        "        # Regular expression to match the package name and optional version specifier\n",
        "        package_pattern = re.compile(r'([a-zA-Z0-9_\\-]+)(?:([=<>!~]+)([0-9a-zA-Z\\.\\*]+))?')\n",
        "\n",
        "        # Find all matches\n",
        "        package_list = package_pattern.findall(package_string)\n",
        "\n",
        "        # Convert matches to a dictionary with appropriate handling of version info\n",
        "        parsed_packages = {}\n",
        "\n",
        "        sanitize_output = lambda x: x if x not in [None, ''] else None\n",
        "\n",
        "        for pkg in package_list:\n",
        "            # Unpack the tuple, using None for missing values\n",
        "            name = pkg[0]\n",
        "            version_operator = pkg[1] if len(pkg) > 1 else None\n",
        "            version_operator = sanitize_output(version_operator)\n",
        "            version = pkg[2] if len(pkg) > 2 else None\n",
        "            version = sanitize_output(version)\n",
        "\n",
        "            parsed_packages[name] = {\n",
        "                'version_operator': version_operator,\n",
        "                'target_version': version\n",
        "            }\n",
        "\n",
        "        return parsed_packages\n",
        "\n",
        "    def pip_update_check(self, pkg: str, version_operator: Optional[str], target_version: Optional[str]):\n",
        "        \"\"\"\n",
        "        Check if an update is required for a given package based on version info.\n",
        "\n",
        "        Args:\n",
        "            pkg (str): The package name.\n",
        "            version_info (dict): A dictionary containing 'version_operator' and 'version'.\n",
        "            current_version (str | None): The current installed version of the package.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if an update is required, False otherwise.\n",
        "        \"\"\"\n",
        "        current_version = self.get_pip_pkg_version(pkg)\n",
        "        if current_version is None:\n",
        "            # If no current version is specified, assume the package is not installed\n",
        "            return True\n",
        "\n",
        "        # Fetch available versions from PyPI\n",
        "        available_versions = self.get_available_pip_package_versions(pkg)\n",
        "\n",
        "        if target_version is None or str(target_version).lower() == 'latest':\n",
        "            # If no target version is specified, use the latest available version\n",
        "            target_version = available_versions[0]\n",
        "\n",
        "        if version_operator is None:\n",
        "            # If no version operator is specified, assume equality\n",
        "            version_operator = '=='\n",
        "\n",
        "        # Parse current and target versions\n",
        "        current_version_parsed, _ = self.parse_version_safe(current_version)\n",
        "        target_version_parsed, wildcard_flag = self.parse_version_safe(target_version)\n",
        "\n",
        "        if current_version_parsed is None or target_version_parsed is None:\n",
        "            raise ValueError(f\"Cannot compare versions: {current_version} and {target_version}\")\n",
        "\n",
        "        # If there is a wildcard in the target version, only compare the relevant parts\n",
        "        if wildcard_flag:\n",
        "            if version_operator == \"==\":\n",
        "                return not str(current_version_parsed).startswith(str(target_version_parsed))\n",
        "            elif version_operator == \"!=\":\n",
        "                return str(current_version_parsed).startswith(str(target_version_parsed))\n",
        "            else:\n",
        "                # Handle other operators if needed with wildcard, but here we focus on equality\n",
        "                return self.compare_versions(current_version_parsed, target_version_parsed, version_operator)\n",
        "\n",
        "        # Standard version comparison for other operators\n",
        "        should_update = self.compare_versions(current_version_parsed, target_version_parsed, version_operator)\n",
        "\n",
        "        # Decision-making based on operator\n",
        "        if version_operator in [\"==\"]:\n",
        "            return not should_update  # Return True if the versions don't match (needs update)\n",
        "        elif version_operator in [\"!=\", \"<\", \">\", \"<=\", \">=\"]:\n",
        "            return should_update  # Return True if the current version doesn't satisfy the target\n",
        "\n",
        "        return False  # Default to no update if something unexpected occurs\n",
        "\n",
        "    def _make_pip_string(self, pkg: str, version_operator: Optional[str], target_version: Optional[str]):\n",
        "        if version_operator is None:\n",
        "            version_operator = '=='\n",
        "        if target_version is None:\n",
        "            target_version = self.get_latest_pip_package_versions(pkg)\n",
        "        return f\"{pkg}{version_operator}{target_version}\".format(pkg=pkg, version_operator=version_operator, target_version=target_version)\n",
        "\n",
        "    def reload(self):\n",
        "        importlib.reload(pkg_resources)\n",
        "\n",
        "    def run_pip_install(self, command: str, background: bool=False):\n",
        "        package_list = buddy.parse_pip_package_string(command)\n",
        "        to_install = [self._make_pip_string(pkg, **version_info) for pkg, version_info in package_list.items() if self.pip_update_check(pkg=pkg, **version_info)]\n",
        "\n",
        "        print(to_install)\n",
        "        if len(to_install) == 0:\n",
        "            return\n",
        "        filtered_packages = \" \".join(to_install)\n",
        "        filtered_command = f\"pip install {filtered_packages}\"\n",
        "        # subprocess.run(filtered_command, shell=True)\n",
        "        self.run_command(filtered_command, background=background)\n",
        "        self.reload()\n",
        "        return\n",
        "\n",
        "class AptBuddy(PacManBuddy):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def is_apt_package_installed(package_name):\n",
        "        try:\n",
        "            # Use dpkg-query to check if the package is installed\n",
        "            result = subprocess.run(\n",
        "                ['dpkg-query', '-W', '-f=${Status}', package_name],\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                check=True,\n",
        "                text=True\n",
        "            )\n",
        "\n",
        "            # The output from dpkg-query is 'install ok installed' if the package is installed\n",
        "            if 'install ok installed' in result.stdout:\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "        except subprocess.CalledProcessError:\n",
        "            # The package is not installed if dpkg-query raises an error\n",
        "            return False\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_apt_package_string(command):\n",
        "        \"\"\"\n",
        "        Parse an apt install command to extract package names.\n",
        "\n",
        "        Args:\n",
        "            command (str): The apt install command string.\n",
        "\n",
        "        Returns:\n",
        "            tuple(list: A list of package names, list: A list of arguments.)\n",
        "        \"\"\"\n",
        "        # Ensure the command starts with 'apt install'\n",
        "        command_pieces = command.split(\" \")\n",
        "        expected_pieces = [('apt', 'apt-get'), 'install']\n",
        "        def check_pieces(expected_pieces, command_pieces):\n",
        "            results = []\n",
        "            for piece in expected_pieces:\n",
        "                if isinstance(piece, tuple):\n",
        "                    subresults = []\n",
        "                    for p in piece:\n",
        "                        subresults.append(check_pieces(p, command_pieces))\n",
        "                    if True in subresults:\n",
        "                        results.append(True)\n",
        "                    else:\n",
        "                        results.append(False)\n",
        "                else:\n",
        "                    if piece not in command_pieces:\n",
        "                        results.append(False)\n",
        "                    else:\n",
        "                        results.append(True)\n",
        "            if True in results:\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "        expected_pieces_presnt = check_pieces(expected_pieces, command_pieces)\n",
        "        if expected_pieces_presnt is not True:\n",
        "            raise ValueError(\"The command must start be an 'apt install' or 'apt-get install' command.\")\n",
        "\n",
        "        # Remove 'apt install' and strip any extra spaces\n",
        "        package_string = [cmd_piece.strip() for cmd_piece in command_pieces]\n",
        "        args = []\n",
        "        packages = []\n",
        "        for cmd_piece in command_pieces:\n",
        "            if cmd_piece.startswith('-'):\n",
        "                args.append(cmd_piece)\n",
        "            else:\n",
        "                if cmd_piece not in ['apt', 'apt-get', 'install']:\n",
        "                    packages.append(cmd_piece)\n",
        "\n",
        "        return packages, args\n",
        "\n",
        "    def run_apt_install(self, command: str, background: bool=False):\n",
        "        \"\"\"\n",
        "        Run apt install command to install packages.\n",
        "\n",
        "        Args:\n",
        "            command (str): The apt install command string.\n",
        "        \"\"\"\n",
        "        packages, args = self.parse_apt_package_string(command)\n",
        "        to_install = [package for package in packages if self.is_apt_package_installed(package) is False]\n",
        "        if len(to_install) == 0:\n",
        "            return\n",
        "        command = \" \".join([\"apt\", \"install\", *args, *to_install])\n",
        "        # subprocess.run(command, shell=True)\n",
        "        self.run_command(command, background=background)\n",
        "\n",
        "class CloudFlaredBuddy:\n",
        "    def __init__(self, path='/content'):\n",
        "        self.path = Path(path)\n",
        "        self.cloudflare_prereq_flag = None\n",
        "        self.cloudflag_flag = None\n",
        "\n",
        "    def cloudflared_init(self, flag_name='cloudflare_prereq_flag'):\n",
        "        cloudflare_prereq_flag = self.path / flag_name\n",
        "        if cloudflare_prereq_flag.exists() is not True:\n",
        "            subprocess.run([\"curl\", \"-L\", \"--output\", \"/content/cloudflared.deb\", \"https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\"])\n",
        "            subprocess.run([\"dpkg\", \"-i\", \"/content/cloudflared.deb\"])\n",
        "            cloudflare_prereq_flag.touch()\n",
        "        self.cloudflare_prereq_flag = cloudflare_prereq_flag\n",
        "        self.cloudflag_flag = cloudflare_prereq_flag.parent / 'cloudflag_flag'\n",
        "\n",
        "    def cloudflare_tunnel(self, cloudflare_token=''):\n",
        "        cloudflare_token = os.environ[\"CLOUDFLARE_TOKEN\"] if cloudflare_token == '' else cloudflare_token\n",
        "        if self.cloudflare_prereq_flag is None:\n",
        "            self.cloudflared_init()\n",
        "        if self.cloudflag_flag.exists() is True:\n",
        "            subprocess.run([\"cloudflared\", \"service\", \"uninstall\"])\n",
        "            os.remove(self.cloudflag_flag)\n",
        "        if self.cloudflag_flag.exists() is not True and cloudflare_token != \"\":\n",
        "            subprocess.run([\"cloudflared\", \"service\", \"install\", cloudflare_token])\n",
        "            self.cloudflag_flag.touch()\n",
        "\n",
        "class PathBuddy:\n",
        "    def __init__(self):\n",
        "        self.init_path = self.get_path()\n",
        "\n",
        "    @staticmethod\n",
        "    def add_to_path(path, prepend=False):\n",
        "        path = os.path.expanduser(path)\n",
        "        path = os.path.expandvars(path)\n",
        "        path_paths = [piece for piece in os.environ[\"PATH\"].split(\":\") if piece != path]\n",
        "        if path not in path_paths:\n",
        "            if prepend:\n",
        "                path_paths.insert(0, path)\n",
        "            else:\n",
        "                path_paths.append(path)\n",
        "        os.environ[\"PATH\"] = \":\".join(path_paths)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_path(path, prepend=False):\n",
        "        path = os.path.expanduser(path)\n",
        "        path = os.path.expandvars(path)\n",
        "        path_paths = [piece for piece in os.environ[\"PATH\"].split(\":\") if piece != path]\n",
        "        os.environ[\"PATH\"] = \":\".join(path_paths)\n",
        "\n",
        "    def reset_path(self):\n",
        "        os.environ[\"PATH\"] = \":\".join(self.init_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_path() -> List[str]:\n",
        "        return os.environ[\"PATH\"].split(\":\")\n",
        "\n",
        "class PythonPathBuddy:\n",
        "    def __init__(self):\n",
        "        self.init_pythonpath = self.get_pythonpath()\n",
        "\n",
        "    @staticmethod\n",
        "    def add_to_pythonpath(path, prepend=False):\n",
        "        path = os.path.expanduser(path)\n",
        "        path = os.path.expandvars(path)\n",
        "        path_paths = [piece for piece in os.environ[\"PYTHONPATH\"].split(\":\") if piece != path]\n",
        "        if path not in path_paths:\n",
        "            if prepend:\n",
        "                path_paths.insert(0, path)\n",
        "            else:\n",
        "                path_paths.append(path)\n",
        "        os.environ[\"PYTHONPATH\"] = \":\".join(path_paths)\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_pythonpath(path, prepend=False):\n",
        "        path = os.path.expanduser(path)\n",
        "        path = os.path.expandvars(path)\n",
        "        path_paths = [piece for piece in os.environ[\"PYTHONPATH\"].split(\":\") if piece != path]\n",
        "        os.environ[\"PYTHONPATH\"] = \":\".join(path_paths)\n",
        "\n",
        "    def reset_pythonpath(self):\n",
        "        os.environ[\"PYTHONPATH\"] = \":\".join(self.init_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_pythonpath() -> List[str]:\n",
        "        return os.environ[\"PYTHONPATH\"].split(\":\")\n",
        "\n",
        "class FileSystemBuddy(BaseBuddy):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @classmethod\n",
        "    def unzip(cls, source: Path | str, destination: Path | None = None):\n",
        "        source = Path(source)\n",
        "        if destination is None:\n",
        "            destination = source.parent / source.stem\n",
        "        destination = Path(destination)\n",
        "        if not destination.exists():\n",
        "            command = \" \".join([\"unzip\", \"-q\", str(source), \"-d\", str(destination.parent)])\n",
        "            cls.run_command(command)\n",
        "            print('Unzipped {source} to {destination}'.format(source=str(source.name), destination=str(destination.name)))\n",
        "\n",
        "    @classmethod\n",
        "    def chmod(cls, args: List[str] | str, path: Path | str):\n",
        "        path = Path(path)\n",
        "        if not path.exists():\n",
        "            raise FileNotFoundError(f\"File not found: {path}\")\n",
        "        if isinstance(args, str):\n",
        "            args = args.split(' ')\n",
        "        final_command = \" \".join([\"chmod\", *args, str(path.absolute())])\n",
        "        cls.run_command(final_command)\n",
        "\n",
        "    @staticmethod\n",
        "    def rm(path: Path | str, recursive=False):\n",
        "        path = Path(path)\n",
        "        if not path.exists():\n",
        "            raise FileNotFoundError(f\"File not found: {path}\")\n",
        "        if not recursive and path.is_dir():\n",
        "            raise ValueError(f\"{path} is a directory. Use recursive=True to remove it.\")\n",
        "        command_stack = [\"rm\", str(path.absolute())]\n",
        "        if recursive:\n",
        "            command_stack.insert(1, '-rf')\n",
        "        subprocess.run(command_stack)\n",
        "        print('Removed {target} from {parent}'.format(target=str(path.name), parent=str(path.parent.absolute())))\n",
        "\n",
        "    @staticmethod\n",
        "    def cd(path: Path | str):\n",
        "        os.chdir(path)\n",
        "        print('Changed directory to {path}'.format(path=str(path)))\n",
        "\n",
        "class LocalRemoteFilePair:\n",
        "    def __init__(self, local_path: Path | str, remote_path: Path | str):\n",
        "        self.local_path = Path(local_path)\n",
        "        self.remote_path = Path(remote_path)\n",
        "\n",
        "    @property\n",
        "    def local(self):\n",
        "        return self.local_path.absolute()\n",
        "\n",
        "    @property\n",
        "    def remote(self):\n",
        "        return self.remote_path.absolute()\n",
        "\n",
        "    def local_exists(self):\n",
        "        return self.local_path.exists()\n",
        "\n",
        "    def remote_exists(self):\n",
        "        return self.remote_path.exists()\n",
        "\n",
        "    def get_local_path(self):\n",
        "        return self.local_path\n",
        "\n",
        "    def get_remote_path(self):\n",
        "        return self.remote_path\n",
        "\n",
        "    @staticmethod\n",
        "    def copy(source, destination):\n",
        "        source = Path(source).absolute()\n",
        "        destination = Path(destination).absolute()\n",
        "        if source.is_dir():\n",
        "            shutil.copytree(str(source), str(destination))  # Copy directory\n",
        "        else:\n",
        "            shutil.copy2(str(source), str(destination))  # Copy file\n",
        "\n",
        "    def copy_to_local(self):\n",
        "        if self.remote_exists():\n",
        "            self.copy(self.remote, self.local)\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"File not found: {self.remote_path}\")\n",
        "\n",
        "    def copy_to_remote(self):\n",
        "        if self.local_exists():\n",
        "            self.copy(self.local, self.remote)\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"File not found: {self.local_path}\")\n",
        "\n",
        "    def remove_local(self):\n",
        "        if self.local_exists():\n",
        "            self.local_path.unlink()\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"File not found: {self.local_path}\")\n",
        "\n",
        "    def remove_remote(self):\n",
        "        if self.remote_exists():\n",
        "            self.remote_path.unlink()\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"File not found: {self.remote_path}\")\n",
        "\n",
        "class GDriveBuddy:\n",
        "    def __init__(self, mount_path: Path | str = '/content/drive'):\n",
        "        self.mount_path = Path(mount_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def connect_gdrive(mount_path: Path | str = '/content/drive'):\n",
        "        mount_path = Path(mount_path)\n",
        "        if mount_path.exists() is not True:\n",
        "            drive.mount(str(mount_path), force_remount=True, readonly=False)\n",
        "        return  mount_path / 'MyDrive'\n",
        "\n",
        "    @staticmethod\n",
        "    def disconnect_gdrive():\n",
        "        drive.flush_and_unmount()\n",
        "        print('GDrive disconnected')\n",
        "\n",
        "    def file_pair(self, local_path: Path | str, remote_path: Path | str):\n",
        "        return LocalRemoteFilePair(local_path, remote_path)\n",
        "\n",
        "class DownloadBuddy(BaseBuddy):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @classmethod\n",
        "    def download_curl(cls, url: str, path: Path | str, *args: str, overwrite=False, follow_redirect=True, **kwargs):\n",
        "        path = Path(path)\n",
        "        redirect_flag_args = False\n",
        "        for arg in args:\n",
        "            dashless_arg = arg.replace('-', '')\n",
        "            if dashless_arg == 'L' or dashless_arg == 'location':\n",
        "                redirect_flag_args = True\n",
        "                break\n",
        "        else:\n",
        "            redirect_flag_args = False\n",
        "\n",
        "        if redirect_flag_args is not True and follow_redirect is not False:\n",
        "            args = ['-L', *args]\n",
        "\n",
        "        def remove_leading_dashes(arg):\n",
        "            while arg.startswith('-'):\n",
        "                arg = arg[1:]\n",
        "            return arg\n",
        "\n",
        "        def arg_processor(arg):\n",
        "            if len(arg) == 1:\n",
        "                arg = '-' + arg\n",
        "            else:\n",
        "                arg = '--' + arg\n",
        "            return arg\n",
        "\n",
        "        def kwargs_processor(key, value):\n",
        "            match(value):\n",
        "                case bool():\n",
        "                    value = str(value).lower()\n",
        "                case str():\n",
        "                    value = value\n",
        "                case Path():\n",
        "                    value = str(value.absolute())\n",
        "                case _:\n",
        "                    value = str(value)\n",
        "            return \"--{key} {value}\".format(key=key, value=value)\n",
        "        args = [remove_leading_dashes(arg) for arg in args]\n",
        "        args = [arg_processor(arg) for arg in args]\n",
        "        kwargs = [kwargs_processor(key, value) for key, value in kwargs.items()]\n",
        "        if path.exists() is not True or overwrite is True:\n",
        "            path.mkdir(parents=True, exist_ok=True)\n",
        "            command = \" \".join([\"curl\", *args, *kwargs, \"--output\", str(path), url])\n",
        "            cls.run_command(command)\n",
        "        else:\n",
        "            print(f\"File already exists: {path}\")\n",
        "\n",
        "    @classmethod\n",
        "    def download_aria2(cls, url: str, path: Path | str, *args: str, max_connections_per_server: int=4, max_concurrent_downloads: int=4, overwrite=False, **kwargs):\n",
        "        path = Path(path)\n",
        "\n",
        "        def arg_preprocessor(arg):\n",
        "            while arg.startswith('-'):\n",
        "                arg = arg[1:]\n",
        "            return arg\n",
        "\n",
        "        def arg_processor(arg):\n",
        "            if len(arg) == 1:\n",
        "                arg = '-' + arg\n",
        "            else:\n",
        "                arg = '--' + arg\n",
        "            return arg\n",
        "\n",
        "        def kwargs_processor(key, value):\n",
        "            key = arg_preprocessor(key)\n",
        "            match(value):\n",
        "                case bool():\n",
        "                    value = str(value).lower()\n",
        "                case str():\n",
        "                    value = value\n",
        "                case Path():\n",
        "                    value = str(value.absolute())\n",
        "                case _:\n",
        "                    value = str(value)\n",
        "            return \"--{key}={value}\".format(key=key, value=value)\n",
        "\n",
        "        default_args = [\n",
        "            \"max-connection-per-server\",\n",
        "            \"max-concurrent-downloads\",\n",
        "            \"dir\",\n",
        "            \"out\"\n",
        "        ]\n",
        "\n",
        "        def default_command_present(command_piece):\n",
        "            if any(arg == command_piece for arg in default_args):\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "\n",
        "        args = [arg_preprocessor(arg) for arg in args]\n",
        "        args = [arg_processor(arg) for arg in args if default_command_present(arg) is not True]\n",
        "        kwargs = [kwargs_processor(key, value) for key, value in kwargs.items() if default_command_present(key) is not True]\n",
        "\n",
        "        command_pieces = [\n",
        "            \"aria2c\",\n",
        "            *args,\n",
        "            *kwargs,\n",
        "            f\"--max-connection-per-server={max_connections_per_server}\",\n",
        "            f\"--max-concurrent-downloads={max_concurrent_downloads}\",\n",
        "            f\"--dir={path.parent}\",\n",
        "            f\"--out={path.name}\",\n",
        "            f\"{url}\"\n",
        "        ]\n",
        "        command = \" \".join(command_pieces)\n",
        "        if path.exists() is not True or overwrite is True:\n",
        "            print(command)\n",
        "            path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            cls.run_command(command)\n",
        "        else:\n",
        "            print(f\"File already exists: {path}\")\n",
        "\n",
        "class MatrixBuddy:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def stringify(item):\n",
        "        \"\"\"\n",
        "        Convert tensor-like objects to their shape representation, or stringify other items.\n",
        "\n",
        "        Args:\n",
        "        item: An item to stringify, potentially a tensor-like object.\n",
        "        \"\"\"\n",
        "        if \"shape\" in dir(item):\n",
        "            if callable(item.shape):\n",
        "                return str(item.shape())\n",
        "            else:\n",
        "                return str(item.shape)\n",
        "        else:\n",
        "            return str(item)\n",
        "\n",
        "    @classmethod\n",
        "    def shapify_strings(cls, iterable, indent=0, last=True):\n",
        "        \"\"\"\n",
        "        Recursively process the contents of an iterable containing other iterables or tensor-like objects,\n",
        "        returning a new iterable with the shapes of tensor-like objects or the same values otherwise,\n",
        "        structured to represent the hierarchy visually.\n",
        "\n",
        "        Args:\n",
        "        iterable (iterable): An iterable containing other iterables or tensor-like objects.\n",
        "        indent (int): The current indentation level, used internally by recursion.\n",
        "        last (bool): Flag to indicate if the current item is the last in its iterable, used to format the tree.\n",
        "        \"\"\"\n",
        "        indent_str = \" \" * (indent * 4)\n",
        "        if isinstance(iterable, dict):\n",
        "            new_dict = {}\n",
        "            for index, (key, value) in enumerate(iterable.items()):\n",
        "                is_last = index == len(iterable) - 1\n",
        "                if isinstance(value, (list, tuple, dict)):\n",
        "                    processed_value = cls.shapify_strings(value, indent + 1, is_last)\n",
        "                else:\n",
        "                    processed_value = cls.stringify(value)\n",
        "                new_dict[key] = processed_value\n",
        "            return new_dict\n",
        "        elif isinstance(iterable, (list, tuple)):\n",
        "            processed_items = []\n",
        "            for index, item in enumerate(iterable):\n",
        "                is_last = index == len(iterable) - 1\n",
        "                if isinstance(item, (list, tuple, dict)):\n",
        "                    processed_item = cls.shapify_strings(item, indent + 1, is_last)\n",
        "                else:\n",
        "                    processed_item = cls.stringify(item)\n",
        "                processed_items.append(processed_item)\n",
        "            return type(iterable)(processed_items)\n",
        "        else:\n",
        "            return cls.stringify(iterable)\n",
        "\n",
        "    @classmethod\n",
        "    def print_shape(cls, item):\n",
        "        pprint(cls.shapify_strings(item))\n",
        "\n",
        "class ColabBuddy(DownloadBuddy, GDriveBuddy, FileSystemBuddy, PathBuddy, PythonPathBuddy, PipBuddy, AptBuddy, CloudFlaredBuddy, RepoBuddy):\n",
        "    def __init__(self, home='/content', drive_path='/content/drive'):\n",
        "        path = home\n",
        "        self.home = Path(home)\n",
        "        DownloadBuddy.__init__(self)\n",
        "        GDriveBuddy.__init__(self, mount_path=drive_path)\n",
        "        FileSystemBuddy.__init__(self)\n",
        "        PathBuddy.__init__(self)\n",
        "        PythonPathBuddy.__init__(self)\n",
        "        PipBuddy.__init__(self)\n",
        "        AptBuddy.__init__(self)\n",
        "        CloudFlaredBuddy.__init__(self, path=home)\n",
        "        RepoBuddy.__init__(self)\n",
        "\n",
        "    @staticmethod\n",
        "    def soft_reset_runtime():\n",
        "        os.execv(sys.executable, ['python'] + sys.argv)\n",
        "\n",
        "    @classmethod\n",
        "    def store_env(cls, name: str, env_dict: dict):\n",
        "        drive_path = cls.connect_gdrive()\n",
        "        colab_buddy_path = drive_path / 'ColabBuddy'\n",
        "        colab_buddy_path.mkdir(exist_ok=True, parents=True)\n",
        "        json_string = json.dumps(env_dict)\n",
        "        with open(colab_buddy_path / f'{name}_env.json', 'w') as f:\n",
        "            f.write(json_string)\n",
        "\n",
        "    @classmethod\n",
        "    def load_env(cls, name: str):\n",
        "        drive_path = cls.connect_gdrive()\n",
        "        colab_buddy_path = drive_path / 'ColabBuddy'\n",
        "        colab_buddy_path.mkdir(exist_ok=True, parents=True)\n",
        "        with open(colab_buddy_path / f'{name}_env.json', 'r') as f:\n",
        "            json_string = f.read()\n",
        "        env_dict = json.loads(json_string)\n",
        "        for key, value in env_dict.items():\n",
        "            os.environ[key] = value\n",
        "\n",
        "    @staticmethod\n",
        "    def flagged_function(callable: Callable, path: Path):\n",
        "        \"\"\"\n",
        "        Checks if a file exists at the path location.\n",
        "        If it does exist, don't run the function.\n",
        "        If it does not exist, run the function and create the file if needed with Path(path).touch().\n",
        "\n",
        "        Parameters:\n",
        "        callable (Callable): a python function or colab shell command wrapped in a python function.\n",
        "        path (Path): The path for the flag file.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "        if not path.exists():\n",
        "            callable()\n",
        "            path.touch()\n",
        "\n",
        "    def go_home(self):\n",
        "        self.cd(self.home)\n",
        "\n",
        "    def set_home(self, path: Path | str):\n",
        "        self.home = Path(path)\n",
        "        print('Set home directory to {path}'.format(path=str(self.home)))"
      ],
      "metadata": {
        "id": "Qnmz0tr9sSeP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title setup\n",
        "# https://github.com/peterljq/Parsimonious-Concept-Engineering\n",
        "buddy = ColabBuddy()\n",
        "buddy.connect_gdrive()\n",
        "buddy.run_pip_install('pip install aioshutil aiosqlite icecream')\n",
        "from icecream import ic\n",
        "remote_project_base = Path(\"/content/drive/MyDrive/Projects/Control_Vectors\")\n",
        "local_project_base = Path(\"/content\")\n",
        "project_base = buddy.file_pair(\n",
        "    local_path=local_project_base,\n",
        "    remote_path=remote_project_base\n",
        ")\n",
        "remote_hd5_file = project_base.remote / \"Parsimonious-Concept-Engineering.hdf5\"\n",
        "hd5_file = project_base.local / \"Parsimonious-Concept-Engineering.hdf5\"\n",
        "hd5_file_pair = buddy.file_pair(\n",
        "    local_path=hd5_file,\n",
        "    remote_path=remote_hd5_file\n",
        ")\n",
        "if not hd5_file_pair.remote_exists():\n",
        "    print(\"Precomputed dataset not found.\\nInstalling prereqs.\")\n",
        "    Parsimonious_Concept_Engineering = buddy.git_clone('https://github.com/peterljq/Parsimonious-Concept-Engineering.git', '/content')\n",
        "    Parsimonious_Concept_Engineering.cd()\n",
        "    buddy.unzip('/content/Parsimonious-Concept-Engineering/concept.zip')\n",
        "    buddy.chmod('+x', 'pace1m_reader.py')\n",
        "    buddy.go_home()\n",
        "    buddy.cd('/content/Parsimonious-Concept-Engineering')\n",
        "    # buddy.rm('Parsimonious-Concept-Engineering', recursive=True)\n",
        "else:\n",
        "    print(\"Precomputed dataset found.\")\n",
        "    hd5_file_pair.copy_to_local()"
      ],
      "metadata": {
        "id": "8ZmdJdBnsVy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load Model\n",
        "from transformers.generation import GenerationConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "def model_changed():\n",
        "  global model_name\n",
        "  global last_model_name\n",
        "  if \"last_model_name\" not in globals():\n",
        "      last_model_name = model_name\n",
        "  if model_name != last_model_name:\n",
        "      last_model_name = model_name\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "import re\n",
        "\n",
        "def make_filename_safe(filename: str) -> str:\n",
        "    \"\"\"\n",
        "    Make a string safe to use as a filename on both Unix and Windows systems\n",
        "    by replacing invalid characters with valid ASCII lookalikes where possible.\n",
        "\n",
        "    Parameters:\n",
        "    filename (str): The input string to be sanitized.\n",
        "\n",
        "    Returns:\n",
        "    str: A sanitized version of the input string.\n",
        "    \"\"\"\n",
        "\n",
        "    # Mapping of invalid characters to valid ASCII lookalikes\n",
        "    char_map = {\n",
        "        '<': '(',  # Less than becomes left paren\n",
        "        '>': ')',  # Greater than becomes right paren\n",
        "        ':': '-',  # Colon becomes dash\n",
        "        '\"': \"'\",  # Double quote becomes single quote\n",
        "        '/': '-',  # Forward slash becomes dash\n",
        "        '\\\\': '-', # Backslash becomes dash\n",
        "        '|': '-',  # Vertical bar becomes dash\n",
        "        '?': '',   # Question mark is removed\n",
        "        '*': 'x',  # Asterisk becomes 'x'\n",
        "        '\\0': '',  # Null byte is removed\n",
        "    }\n",
        "\n",
        "    # Function to replace invalid characters using the map\n",
        "    def replace_invalid_char(match):\n",
        "        return char_map.get(match.group(0), '_')  # Default to '_' if no mapping is found\n",
        "\n",
        "    # Characters not allowed in filenames\n",
        "    invalid_chars = r'[<>:\"/\\\\|?*\\0]'\n",
        "\n",
        "    # Replace invalid characters with ASCII lookalikes\n",
        "    safe_filename = re.sub(invalid_chars, replace_invalid_char, filename)\n",
        "\n",
        "    # Strip leading and trailing spaces or periods (Windows restriction)\n",
        "    safe_filename = safe_filename.strip(' .')\n",
        "\n",
        "    # Ensure the filename is not empty after cleaning\n",
        "    if not safe_filename:\n",
        "        safe_filename = 'default_filename'\n",
        "\n",
        "    return safe_filename\n",
        "\n",
        "default_model_name = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
        "model_name = 'microsoft/Phi-3-mini-4k-instruct' # @param {type:'string'}\n",
        "if model_name == '':\n",
        "    model_name = default_model_name\n",
        "filesafe_model_name = make_filename_safe(model_name)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the LLM\n",
        "if \"model\" not in locals() or model_changed():\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    model.to(device)\n",
        "    print(\"Model loaded.\")\n",
        "else:\n",
        "    print(\"Model already loaded.\")\n",
        "\n",
        "if \"tokenizer\" not in locals() or model_changed():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    print(\"Tokenizer loaded.\")\n",
        "else:\n",
        "    print(\"Tokenizer already loaded.\")"
      ],
      "metadata": {
        "id": "JzUlt8QYKiv1",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3svkUna2dfNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I admittedly, have no formal ML education, however that has not stopped me from learning through free educational resources, and experimentation.\n",
        "\n",
        "These are utility functions that allow me to print nested data structures containing torch tensors and numpy arrays, where the tensors/matrices are printed as shapes, so that I can more effectively view the dimensionality of my data with simple print statements in this notebook."
      ],
      "metadata": {
        "id": "a6p9jAgykPyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Utility code\n",
        "\n",
        "from pprint import PrettyPrinter\n",
        "from pprint import pprint as _pprint\n",
        "from collections.abc import Mapping, Iterable\n",
        "import torch\n",
        "\n",
        "# Global flag to indicate if we are inside a pretty print call\n",
        "globals()[\"_is_print\"] = False\n",
        "\n",
        "# Override the pprint.pprint function to use the custom printer\n",
        "def pprint(*args, **kwargs):\n",
        "    globals()[\"_is_print\"] = True\n",
        "    _pprint(*args, **kwargs)\n",
        "    globals()[\"_is_print\"] = False\n",
        "\n",
        "def reassign_print(reset=False):\n",
        "    if \"print_original\" not in globals():\n",
        "        globals()[\"print_original\"] = print  # Store original print function\n",
        "\n",
        "    if \"_is_print\" not in globals():\n",
        "        globals()[\"_is_print\"] = False  # Flag to indicate if custom print is being used\n",
        "\n",
        "    if reset:\n",
        "        # Restore the original print function\n",
        "        globals()['print'] = globals()[\"print_original\"]\n",
        "        return\n",
        "\n",
        "    def flagged_print(*args, **kwargs):\n",
        "        globals()[\"_is_print\"] = True  # Set the flag to True before printing\n",
        "        try:\n",
        "            # Call the original print function with the passed arguments\n",
        "            globals()[\"print_original\"](*args, **kwargs)\n",
        "        finally:\n",
        "            globals()[\"_is_print\"] = False  # Reset the flag to False after printing\n",
        "\n",
        "    globals()[\"flagged_print\"] = flagged_print\n",
        "\n",
        "    # Reassign print to the custom function\n",
        "    globals()['print'] = globals()[\"flagged_print\"]\n",
        "    return\n",
        "\n",
        "reassign_print()\n",
        "\n",
        "class PrefixedDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        if not hasattr(self, '_prefix'):\n",
        "            self._prefix = \"\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self._cache = None  # Cache for the prefixed dictionary\n",
        "\n",
        "    @property\n",
        "    def dict(self):\n",
        "        if globals().get('_is_print', False):\n",
        "            return self.get_prefixed()\n",
        "        else:\n",
        "            return dict(super().items())\n",
        "\n",
        "    def to_dict(self):\n",
        "        output = {}\n",
        "        for key, value in self.items():\n",
        "            if isinstance(value, PrefixedDict):\n",
        "                value = value.to_dict()\n",
        "            output[key] = value\n",
        "        return output\n",
        "\n",
        "    def _process_key(self, key):\n",
        "        \"\"\"\n",
        "        Returns the raw key and the prefixed key.\n",
        "        \"\"\"\n",
        "        raw_key = key\n",
        "        prefixed_key = f\"{self._prefix}{key}\"\n",
        "        return raw_key, prefixed_key\n",
        "\n",
        "    def get_prefixed(self):\n",
        "        \"\"\"\n",
        "        Returns a dictionary with prefixed keys, caching it for efficiency.\n",
        "        \"\"\"\n",
        "        if self._cache is not None:\n",
        "            return self._cache\n",
        "\n",
        "        # Create a prefixed version of the dictionary for display\n",
        "        prefixed_dict = self.shapify_tensors({f\"{self._prefix}{key}\": value for key, value in super().items()})\n",
        "        self._cache = prefixed_dict\n",
        "        return prefixed_dict\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        # Invalidate the cache on updates\n",
        "        self._cache = None\n",
        "        super().__setitem__(key, value)\n",
        "\n",
        "    def __delitem__(self, key):\n",
        "        # Invalidate the cache on deletions\n",
        "        self._cache = None\n",
        "        super().__delitem__(key)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return super().__getitem__(key)\n",
        "\n",
        "    def __contains__(self, key):\n",
        "        return super().__contains__(key)\n",
        "\n",
        "    def items(self):\n",
        "        return self.dict.items()\n",
        "\n",
        "    def keys(self):\n",
        "        return super().keys()\n",
        "\n",
        "    def values(self):\n",
        "        return super().values()\n",
        "\n",
        "    @classmethod\n",
        "    def shapify_tensors(cls, value):\n",
        "        \"\"\"\n",
        "        If an item is a PyTorch tensor, convert its value to its shape string.\n",
        "        \"\"\"\n",
        "        def shapify(value):\n",
        "            if \"shape\" in dir(value):\n",
        "                if callable(value.shape):\n",
        "                    shape_dims = [str(dim) for dim in list(value.shape())]\n",
        "                else:\n",
        "                    shape_dims = [str(dim) for dim in list(value.shape)]\n",
        "                return 'Tensor.shape(' + \", \".join(shape_dims) + ')'\n",
        "            else:\n",
        "                return value\n",
        "\n",
        "        return cls.apply_func_recursively(value, shapify)\n",
        "\n",
        "    @classmethod\n",
        "    def apply_func_recursively(cls, data, func):\n",
        "        if isinstance(data, Mapping):\n",
        "            # Apply the function recursively to dictionary values\n",
        "            return {key: cls.apply_func_recursively(value, func) for key, value in data.items()}\n",
        "        elif isinstance(data, Iterable) and not isinstance(data, (str, bytes)) and not \"shape\" in dir(data):\n",
        "            # Apply the function recursively to each element in an iterable (list, tuple, etc.)\n",
        "            return type(data)([cls.apply_func_recursively(element, func) for element in data])\n",
        "        else:\n",
        "            # Base case: Apply the function to the scalar value\n",
        "            return func(data)\n",
        "\n",
        "class LayersDict(PrefixedDict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self._prefix=\"layer_\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "class PromptDict(PrefixedDict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self._prefix=\"prompt_\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "class TokenDict(PrefixedDict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self._prefix=\"token_\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "# Testing the updated class\n",
        "sample_dict = {}\n",
        "for i in range(5):\n",
        "    sample_dict[str(i)] =  LayersDict(**{str(i): torch.rand(2,3,4) for i in range(2)})\n",
        "\n",
        "# Create a LayersDict instance\n",
        "layers_dict = LayersDict(sample_dict)\n",
        "\n",
        "# Pretty print the LayersDict instance\n",
        "pprint(layers_dict, compact=False, indent=4, underscore_numbers=True)\n"
      ],
      "metadata": {
        "id": "PwLtlS5xooj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I don't have infinite memory, making efficient use of resources is vital when you only have the free tier to work with.\n",
        "\n",
        "The following classes offer synchronous and asynchronous methods for storing tensors from model activations in-memory or on-disk"
      ],
      "metadata": {
        "id": "ECS1c1Nil4qK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Sync DB Managers\n",
        "import sqlite3\n",
        "import numpy as np\n",
        "import torch\n",
        "from io import BytesIO\n",
        "\n",
        "class SQLiteBuddy:\n",
        "    def __init__(self, db_path):\n",
        "        \"\"\"Initialize with the database file path.\"\"\"\n",
        "        self.db_path = db_path\n",
        "        self.connection = None\n",
        "        self.cursor = None\n",
        "        self.connect()\n",
        "\n",
        "    def connect(self):\n",
        "        \"\"\"Establish a connection to the SQLite database.\"\"\"\n",
        "        try:\n",
        "            self.connection = sqlite3.connect(self.db_path)\n",
        "            self.cursor = self.connection.cursor()\n",
        "            print(\"Connected to the database successfully.\")\n",
        "        except sqlite3.Error as e:\n",
        "            print(f\"Error connecting to the database: {e}\")\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close the connection to the SQLite database.\"\"\"\n",
        "        if self.connection:\n",
        "            self.connection.close()\n",
        "            print(\"Connection closed.\")\n",
        "\n",
        "    def execute_query(self, query, params=()):\n",
        "        \"\"\"Execute a single query.\"\"\"\n",
        "        try:\n",
        "            self.cursor.execute(query, params)\n",
        "            self.connection.commit()\n",
        "            print(\"Query executed successfully.\")\n",
        "        except sqlite3.Error as e:\n",
        "            print(f\"Error executing query: {e}\")\n",
        "\n",
        "    def execute_many(self, query, params_list):\n",
        "        \"\"\"Execute multiple queries with varying parameters.\"\"\"\n",
        "        try:\n",
        "            self.cursor.executemany(query, params_list)\n",
        "            self.connection.commit()\n",
        "            print(\"Multiple queries executed successfully.\")\n",
        "        except sqlite3.Error as e:\n",
        "            print(f\"Error executing multiple queries: {e}\")\n",
        "\n",
        "    def fetch_all(self, query, params=()):\n",
        "        \"\"\"Fetch all results from a query.\"\"\"\n",
        "        try:\n",
        "            self.cursor.execute(query, params)\n",
        "            results = self.cursor.fetchall()\n",
        "            return results\n",
        "        except sqlite3.Error as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def fetch_one(self, query, params=()):\n",
        "        \"\"\"Fetch one result from a query.\"\"\"\n",
        "        try:\n",
        "            self.cursor.execute(query, params)\n",
        "            result = self.cursor.fetchone()\n",
        "            return result\n",
        "        except sqlite3.Error as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_table(self, table_name, columns):\n",
        "        \"\"\"Create a table with the specified columns.\"\"\"\n",
        "        try:\n",
        "            column_defs = \", \".join([f\"{col_name} {col_type}\" for col_name, col_type in columns.items()])\n",
        "            create_table_query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({column_defs});\"\n",
        "            self.execute_query(create_table_query)\n",
        "            print(f\"Table '{table_name}' created successfully.\")\n",
        "        except sqlite3.Error as e:\n",
        "            print(f\"Error creating table: {e}\")\n",
        "\n",
        "    def insert(self, table_name, data):\n",
        "        \"\"\"Insert a row into a table.\"\"\"\n",
        "        columns = \", \".join(data.keys())\n",
        "        placeholders = \", \".join([\"?\" for _ in data.values()])\n",
        "        insert_query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
        "        self.execute_query(insert_query, tuple(data.values()))\n",
        "        print(f\"Data inserted into '{table_name}'.\")\n",
        "\n",
        "    def select_all(self, table_name):\n",
        "        \"\"\"Select all data from a table.\"\"\"\n",
        "        select_query = f\"SELECT * FROM {table_name}\"\n",
        "        return self.fetch_all(select_query)\n",
        "\n",
        "class PaceDbBuddy(SQLiteBuddy):\n",
        "    def __init__(self, db_path):\n",
        "        \"\"\"Initialize the TensorDBBuddy with a database file path.\"\"\"\n",
        "        super().__init__(db_path)\n",
        "        self.create_tensor_table()\n",
        "\n",
        "    def create_tensor_table(self):\n",
        "        \"\"\"Create a table with 'key' as the primary key and 'value' for tensors.\"\"\"\n",
        "        create_table_query = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS tensors (\n",
        "            key TEXT PRIMARY KEY,\n",
        "            value BLOB\n",
        "        );\n",
        "        \"\"\"\n",
        "        self.execute_query(create_table_query)\n",
        "\n",
        "    def insert_tensor(self, concept, tensor: torch.Tensor):\n",
        "        \"\"\"Insert a tensor into the table, converting the tensor to bytes.\"\"\"\n",
        "        np_bytes = BytesIO()\n",
        "        tensor_np = tensor.detach().cpu().numpy().astype(np.dtype('float32')) # Convert tensor to numpy array\n",
        "        np.save(np_bytes, tensor_np, allow_pickle=True)\n",
        "        tensor_bytes = np_bytes.getvalue() # Serialize the numpy array\n",
        "        insert_query = \"INSERT OR REPLACE INTO tensors (key, value) VALUES (?, ?)\"\n",
        "        self.execute_query(insert_query, (concept, tensor_bytes))\n",
        "        print(f\"Tensor for concept '{concept}' inserted successfully.\")\n",
        "\n",
        "    def fetch_tensor(self, concept):\n",
        "        \"\"\"Fetch a tensor from the table, deserializing the bytes back to a PyTorch tensor.\"\"\"\n",
        "        select_query = \"SELECT value FROM tensors WHERE key = ?\"\n",
        "        result = self.fetch_all(select_query, (concept,))\n",
        "        if result and result[0][0]:\n",
        "            tensor_bytes = result[0][0]\n",
        "            load_bytes = BytesIO(tensor_bytes)\n",
        "            loaded_np = np.load(load_bytes, allow_pickle=True) # Deserialize to numpy array\n",
        "            tensor = torch.tensor(loaded_np) # Convert numpy array back to PyTorch tensor\n",
        "            return tensor\n",
        "        else:\n",
        "            print(f\"No tensor found for concept '{concept}'\")\n",
        "            return None\n",
        "\n",
        "    def list_concepts(self):\n",
        "        \"\"\"Retrieve all concepts that have been uploaded (present in the database).\"\"\"\n",
        "        select_query = \"SELECT key FROM tensors\"\n",
        "        result = self.fetch_all(select_query)\n",
        "        if result:\n",
        "            concepts = [row[0] for row in result]\n",
        "            return concepts\n",
        "        else:\n",
        "            print(\"No concepts found.\")\n",
        "            return []"
      ],
      "metadata": {
        "cellView": "form",
        "id": "tXjnzsrZOnMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Async DB Managers\n",
        "import logging\n",
        "from io import BytesIO\n",
        "import asyncio\n",
        "import torch\n",
        "import numpy as np\n",
        "import aiosqlite\n",
        "\n",
        "# Configure dblogger\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,  # Set the logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',  # Define the log format\n",
        "    handlers=[logging.StreamHandler()]  # Output logs to the console (stdout)\n",
        ")\n",
        "\n",
        "dblogger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class AsyncSQLiteBuddy:\n",
        "    def __init__(self, db_path):\n",
        "        self.db_path = db_path\n",
        "\n",
        "    async def execute_query(self, query, params=()):\n",
        "        \"\"\"Execute a single query with optional parameters.\"\"\"\n",
        "        async with aiosqlite.connect(self.db_path) as db:\n",
        "            await db.execute(query, params)\n",
        "            await db.commit()\n",
        "\n",
        "    async def execute_many(self, query, params_list):\n",
        "        \"\"\"Execute multiple queries in one go.\"\"\"\n",
        "        async with aiosqlite.connect(self.db_path) as db:\n",
        "            await db.executemany(query, params_list)\n",
        "            await db.commit()\n",
        "\n",
        "    async def fetch_all(self, query, params=()):\n",
        "        \"\"\"Fetch all rows for a query.\"\"\"\n",
        "        async with aiosqlite.connect(self.db_path) as db:\n",
        "            cursor = await db.execute(query, params)\n",
        "            rows = await cursor.fetchall()\n",
        "            await cursor.close()\n",
        "            return rows\n",
        "\n",
        "    async def fetch_one(self, query, params=()):\n",
        "        \"\"\"Fetch a single row for a query.\"\"\"\n",
        "        async with aiosqlite.connect(self.db_path) as db:\n",
        "            cursor = await db.execute(query, params)\n",
        "            row = await cursor.fetchone()\n",
        "            await cursor.close()\n",
        "            return row\n",
        "\n",
        "    async def create_table(self, table_name, columns):\n",
        "        \"\"\"\n",
        "        Create a table with the given name and columns.\n",
        "\n",
        "        :param table_name: str - Name of the table\n",
        "        :param columns: dict - Column names and types (e.g., {'id': 'INTEGER', 'name': 'TEXT'})\n",
        "        \"\"\"\n",
        "        columns_def = ', '.join(f\"{col} {dtype}\" for col, dtype in columns.items())\n",
        "        query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({columns_def})\"\n",
        "        await self.execute_query(query)\n",
        "\n",
        "    async def insert(self, table_name, data):\n",
        "        \"\"\"\n",
        "        Insert data into the table.\n",
        "\n",
        "        :param table_name: str - Name of the table\n",
        "        :param data: dict - Data to be inserted as a dictionary {column_name: value}\n",
        "        \"\"\"\n",
        "        columns = ', '.join(data.keys())\n",
        "        placeholders = ', '.join(['?' for _ in data.values()])\n",
        "        query = f\"INSERT INTO {table_name} ({columns}) VALUES ({placeholders})\"\n",
        "        params = tuple(data.values())\n",
        "        await self.execute_query(query, params)\n",
        "\n",
        "    async def select_all(self, table_name):\n",
        "        \"\"\"Select all rows from a table.\"\"\"\n",
        "        query = f\"SELECT * FROM {table_name}\"\n",
        "        return await self.fetch_all(query)\n",
        "\n",
        "class AsyncBatchDbBuddy(AsyncSQLiteBuddy):\n",
        "    def __init__(self, db_path, table_name, batch_size=10, flush_interval=5, max_retries=5, retry_delay=30):\n",
        "        \"\"\"\n",
        "        Initialize the AsyncBatchDbBuddy with common parameters for batch insertion.\n",
        "        :param db_path: Path to the database file\n",
        "        :param table_name: The name of the table for this specific buddy\n",
        "        :param batch_size: Maximum batch size before auto-flush\n",
        "        :param flush_interval: Interval to auto-flush (seconds)\n",
        "        :param max_retries: Maximum retries when a database lock is encountered\n",
        "        :param retry_delay: Delay (in seconds) for retrying after lock\n",
        "        \"\"\"\n",
        "        super().__init__(db_path)\n",
        "        self.table_name = table_name\n",
        "        self.batch_size = batch_size  # Maximum batch size before auto-flush\n",
        "        self.flush_interval = flush_interval  # Interval to auto-flush (seconds)\n",
        "        self.insert_queue = asyncio.Queue()  # Queue to store insertions\n",
        "        self.batch_worker_task = None  # Background worker task\n",
        "        self.running = True  # Flag to control the worker loop\n",
        "        self.max_retries = max_retries  # Maximum retries for a database operation\n",
        "        self.retry_delay = retry_delay  # Delay for retrying after lock\n",
        "        self.lock = asyncio.Lock()  # Ensure no concurrent flushes\n",
        "\n",
        "    async def start_batch_worker(self):\n",
        "        \"\"\"Start a background task that handles batching and inserts.\"\"\"\n",
        "        self.batch_worker_task = asyncio.create_task(self.batch_worker())\n",
        "\n",
        "    async def stop_batch_worker(self):\n",
        "        \"\"\"Stop the background task.\"\"\"\n",
        "        self.running = False\n",
        "        if self.batch_worker_task:\n",
        "            await self.batch_worker_task\n",
        "\n",
        "    async def batch_worker(self):\n",
        "        \"\"\"Background worker that collects inserts and batches them.\"\"\"\n",
        "        while self.running:\n",
        "            await asyncio.sleep(self.flush_interval)  # Wait for the flush interval\n",
        "            await self.flush_queue()\n",
        "\n",
        "    async def flush_queue(self):\n",
        "        \"\"\"Flush the queue by inserting accumulated data in a batch, with retry logic.\"\"\"\n",
        "        async with self.lock:  # Ensure no concurrent flushes\n",
        "            batch = []\n",
        "            while not self.insert_queue.empty() and len(batch) < self.batch_size:\n",
        "                batch.append(await self.insert_queue.get())\n",
        "\n",
        "            if batch:\n",
        "                # Create a batched query with multiple rows, using the table name from the subclass\n",
        "                insert_query = f\"INSERT OR REPLACE INTO {self.table_name} (key, value) VALUES (?, ?)\"\n",
        "                await self._execute_with_retry(insert_query, batch)\n",
        "                dblogger.info(f\"Batch of {len(batch)} items inserted successfully into {self.table_name}.\")\n",
        "\n",
        "    async def _execute_with_retry(self, query, params_list):\n",
        "        \"\"\"Execute a query with retry logic for handling database locks.\"\"\"\n",
        "        retries = 0\n",
        "        while retries < self.max_retries:\n",
        "            try:\n",
        "                await self.execute_many(query, params_list)\n",
        "                break  # Break the loop if the query succeeds\n",
        "            except aiosqlite.OperationalError as e:\n",
        "                if \"database is locked\" in str(e):\n",
        "                    retries += 1\n",
        "                    wait_time = self.retry_delay * retries\n",
        "                    dblogger.warning(f\"Database is locked, retrying in {wait_time} seconds...\")\n",
        "                    await asyncio.sleep(wait_time)\n",
        "                else:\n",
        "                    dblogger.error(f\"Database operation failed: {e}\")\n",
        "                    raise e  # Re-raise any other operational errors\n",
        "\n",
        "    async def insert_data(self, key, value):\n",
        "        \"\"\"Queue data for batched insertion.\"\"\"\n",
        "        await self.insert_queue.put((key, value))  # Add to queue\n",
        "\n",
        "        # If queue size exceeds batch size, flush the queue immediately\n",
        "        if self.insert_queue.qsize() >= self.batch_size:\n",
        "            await self.flush_queue()\n",
        "\n",
        "    async def create_table(self, create_table_sql):\n",
        "        \"\"\"Create a table asynchronously using the provided SQL.\"\"\"\n",
        "        await self.execute_query(create_table_sql)\n",
        "\n",
        "    async def fetch_data(self, key):\n",
        "        \"\"\"Fetch data from the table asynchronously.\"\"\"\n",
        "        select_query = f\"SELECT value FROM {self.table_name} WHERE key = ?\"\n",
        "        result = await self.fetch_all(select_query, (key,))\n",
        "        if result and result[0][0]:\n",
        "            return result[0][0]\n",
        "        else:\n",
        "            dblogger.info(f\"No data found for key '{key}'\")\n",
        "            return None\n",
        "\n",
        "    async def list_keys(self):\n",
        "        \"\"\"Retrieve all keys that are present in the database asynchronously.\"\"\"\n",
        "        select_query = f\"SELECT key FROM {self.table_name}\"\n",
        "        result = await self.fetch_all(select_query)\n",
        "        if result:\n",
        "            keys = [row[0] for row in result]\n",
        "            return keys\n",
        "        else:\n",
        "            dblogger.info(\"No keys found.\")\n",
        "            return []\n",
        "\n",
        "class AsyncPaceDbBuddy(AsyncBatchDbBuddy):\n",
        "    def __init__(self, db_path, batch_size=10, flush_interval=5, max_retries=5, retry_delay=1):\n",
        "        self.dtype = np.dtype('float32')\n",
        "        super().__init__(db_path, \"tensors\", batch_size, flush_interval, max_retries, retry_delay)\n",
        "\n",
        "    @classmethod\n",
        "    async def create_db(cls, db_path, batch_size=10, flush_interval=5):\n",
        "        db = cls(db_path=db_path, batch_size=batch_size, flush_interval=flush_interval)\n",
        "        await db.create_tensor_table()\n",
        "        return db\n",
        "\n",
        "    async def create_tensor_table(self):\n",
        "        \"\"\"Create a table for tensors asynchronously.\"\"\"\n",
        "        create_table_query = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS tensors (\n",
        "            key TEXT PRIMARY KEY,\n",
        "            value BLOB\n",
        "        );\n",
        "        \"\"\"\n",
        "        await self.create_table(create_table_query)\n",
        "\n",
        "    async def insert_tensor(self, concept, tensor: torch.Tensor):\n",
        "        \"\"\"Queue a tensor for batched insertion.\"\"\"\n",
        "        np_bytes = BytesIO()\n",
        "        tensor_np = tensor.detach().cpu().numpy().astype(np.dtype('float32')) # Convert tensor to numpy array\n",
        "        np.save(np_bytes, tensor_np, allow_pickle=True)\n",
        "        tensor_bytes = np_bytes.getvalue() # Serialize the numpy array\n",
        "        await self.insert_data(concept, tensor_bytes)\n",
        "\n",
        "    async def fetch_tensor(self, concept):\n",
        "        \"\"\"Fetch a tensor from the table, deserializing the bytes back to a PyTorch tensor asynchronously.\"\"\"\n",
        "        tensor_bytes = await self.fetch_data(concept)\n",
        "        if tensor_bytes:\n",
        "            load_bytes = BytesIO(tensor_bytes)\n",
        "            loaded_np = np.load(load_bytes, allow_pickle=True) # Deserialize to numpy array\n",
        "            tensor = torch.tensor(loaded_np) # Convert numpy array back to PyTorch tensor\n",
        "            return tensor\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    async def list_concepts(self):\n",
        "        \"\"\"Retrieve all keys that are present in the database asynchronously.\"\"\"\n",
        "        return await self.list_keys()\n",
        "\n",
        "class AsyncPaceCacheBuddy(AsyncBatchDbBuddy):\n",
        "    def __init__(self, db_path, batch_size=10, flush_interval=5, max_retries=5, retry_delay=1):\n",
        "        self.dtype = np.dtype('float32')\n",
        "        super().__init__(db_path, \"activation_cache\", batch_size, flush_interval, max_retries, retry_delay)\n",
        "\n",
        "    @classmethod\n",
        "    async def create_db(cls, db_path, batch_size=10, flush_interval=5):\n",
        "        db = cls(db_path=db_path, batch_size=batch_size, flush_interval=flush_interval)\n",
        "        await db.create_cache_table()\n",
        "        return db\n",
        "\n",
        "    async def create_cache_table(self):\n",
        "        \"\"\"Create a table for cache asynchronously.\"\"\"\n",
        "        create_table_query = \"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS activation_cache (\n",
        "            key TEXT PRIMARY KEY,\n",
        "            value BLOB\n",
        "        );\n",
        "        \"\"\"\n",
        "        await self.create_table(create_table_query)\n",
        "\n",
        "    async def insert_cache(self, key, tensor: torch.Tensor):\n",
        "        \"\"\"Queue a tensor for batched insertion.\"\"\"\n",
        "        np_bytes = BytesIO()\n",
        "        tensor_np = tensor.detach().cpu().numpy().astype(np.dtype('float32')) # Convert tensor to numpy array\n",
        "        np.save(np_bytes, tensor_np, allow_pickle=True)\n",
        "        tensor_bytes = np_bytes.getvalue() # Serialize the numpy array\n",
        "        await self.insert_data(key, tensor_bytes)\n",
        "\n",
        "    async def fetch_cache(self, key):\n",
        "        \"\"\"Fetch a tensor from the cache, deserializing the bytes back to a PyTorch tensor asynchronously.\"\"\"\n",
        "        tensor_bytes = await self.fetch_data(key)\n",
        "        if tensor_bytes:\n",
        "            load_bytes = BytesIO(tensor_bytes)\n",
        "            loaded_np = np.load(load_bytes, allow_pickle=True) # Deserialize to numpy array\n",
        "            tensor = torch.tensor(loaded_np) # Convert numpy array back to PyTorch tensor\n",
        "            return tensor\n",
        "        else:\n",
        "            return None"
      ],
      "metadata": {
        "cellView": "form",
        "id": "EIEv_d1UQYPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test AsyncPaceCacheBuddy\n",
        "!rm '/content/test.db'\n",
        "test_db = await AsyncPaceDbBuddy.create_db(db_path='/content/test.db', batch_size=3, flush_interval=3)\n",
        "values = []\n",
        "for i in range(9):\n",
        "    value = torch.rand(52, 2048, 1)\n",
        "    values.append(value)\n",
        "    await test_db.insert_tensor(f\"test{i}\", value)\n",
        "stored_keys = await test_db.list_concepts()\n",
        "assert len(stored_keys) == len(values)\n",
        "for idx, key in enumerate(stored_keys):\n",
        "    data = await test_db.fetch_tensor(key)\n",
        "    torch.testing.assert_close(data, values[idx])"
      ],
      "metadata": {
        "cellView": "form",
        "id": "J3BlBE7w0H13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset functions\n",
        "from more_itertools import chunked\n",
        "from typing import List, Dict, Tuple, Any, Optional\n",
        "import h5py\n",
        "from functools import lru_cache\n",
        "\n",
        "\n",
        "def get_concepts() -> Tuple[str]:\n",
        "    with h5py.File(hd5_file, 'r') as hdf:\n",
        "        concepts = tuple(hdf.keys())\n",
        "    return concepts\n",
        "\n",
        "def get_stimuli(concepts: List[str], restrict_stimuli: Optional[int]=None) -> Tuple[Tuple[str]]:\n",
        "    if restrict_stimuli is not None:\n",
        "        stimuli_slice = slice(0, restrict_stimuli)\n",
        "    else:\n",
        "        stimuli_slice = slice(None)\n",
        "    with h5py.File(hd5_file, 'r') as hdf:\n",
        "        stimuli = tuple((tuple(prompt.decode() for prompt in hdf[concept][stimuli_slice]) for concept in concepts))\n",
        "    return stimuli\n",
        "\n",
        "# LRU can only cache hashable objects, need to convert lists and other mutable input types to a hashable type before caching\n",
        "@lru_cache(maxsize=None)\n",
        "def get_stimuli_len_cached(concepts_json: str) -> int:\n",
        "    concepts = json.loads(concepts_json)\n",
        "    with h5py.File(hd5_file, 'r') as hdf:\n",
        "        return sum(len(hdf[concept]) for concept in concepts)\n",
        "\n",
        "def get_stimuli_len(concepts: List[str]) -> int:\n",
        "    concepts_json = json.dumps(concepts)\n",
        "    return get_stimuli_len_cached(concepts_json)\n",
        "\n",
        "@lru_cache(maxsize=None)\n",
        "def get_all_stimuli_len() -> int:\n",
        "    with h5py.File(hd5_file, 'r') as hdf:\n",
        "        total = sum(len(hdf[concept]) for concept in hdf.keys())\n",
        "    return total\n",
        "\n",
        "def dataset_generator(concepts: List[str], load_n_concepts=3, restrict_stimuli: Optional[int]=None):\n",
        "    for batch_concepts in chunked(concepts, n=load_n_concepts):\n",
        "        stimuli = get_stimuli(batch_concepts, restrict_stimuli)\n",
        "        for concept, prompts in zip(batch_concepts, stimuli):\n",
        "            yield concept, prompts"
      ],
      "metadata": {
        "id": "qaHYaG73sgXo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PaCE Extraction Classes\n",
        "from ctypes import ArgumentError\n",
        "from tqdm.autonotebook import tqdm\n",
        "import asyncio\n",
        "from more_itertools import chunked\n",
        "import inspect\n",
        "\n",
        "from typing import Tuple, List\n",
        "import torch\n",
        "\n",
        "# Type hint for the hidden states, which is a tuple of tuples of torch tensors\n",
        "HiddenStatesType = Tuple[Tuple[torch.Tensor, ...],...]\n",
        "\n",
        "# Type hint for the past key values, which is a tuple of tuples, each containing two torch tensors\n",
        "PastKeyValuesType = Tuple[Tuple[torch.Tensor, torch.Tensor], ...]\n",
        "\n",
        "# Type hint for the overall structure\n",
        "ModelOutputType = dict[str, Tuple[HiddenStatesType, PastKeyValuesType, torch.Tensor]]\n",
        "\n",
        "\n",
        "pacelogger = logging.getLogger(__name__)\n",
        "\n",
        "class RecursiveDict:\n",
        "    def __init__(self):\n",
        "        self._data = {}\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if key not in self._data:\n",
        "            self._data[key] = RecursiveDict()  # Create new RecursiveDict if key doesn't exist\n",
        "        return self._data[key]\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        self._data[key] = value\n",
        "\n",
        "    def __repr__(self):\n",
        "        return repr(self._data)\n",
        "\n",
        "    def keys(self):\n",
        "        return self._data.keys()\n",
        "\n",
        "    def values(self):\n",
        "        return self._data.values()\n",
        "\n",
        "    def items(self):\n",
        "        return self._data.items()\n",
        "\n",
        "class RecursivePrefixedDict(PrefixedDict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self._prefix=\"\"\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        if not super().__contains__(key):\n",
        "            super().__setitem__(key, RecursivePrefixedDict())  # Create new RecursiveDict if key doesn't exist\n",
        "        return super().__getitem__(key)\n",
        "\n",
        "    def keys(self):\n",
        "        return super().keys()\n",
        "\n",
        "    def values(self):\n",
        "        return super().values()\n",
        "\n",
        "    def items(self):\n",
        "        return super().items()\n",
        "\n",
        "class LlmBuddy:\n",
        "    def __init__(self):\n",
        "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "        import torch\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(prompts, tokenizer, device):\n",
        "        inputs = tokenizer(prompts, padding=True, return_tensors='pt')\n",
        "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "        return inputs\n",
        "\n",
        "    @staticmethod\n",
        "    def generate(inputs, model, gen_cfg):\n",
        "        return model.generate(\n",
        "            inputs.get(\"input_ids\"),\n",
        "            attention_mask=inputs.get(\"attention_mask\"),\n",
        "            generation_config=gen_cfg,\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def _padding_args(pad_lengths: Optional[List[int]]=None, pad_direction: Optional[Literal[\"left\", \"right\"]]=None, remove_padding: bool=False):\n",
        "        if (pad_lengths is not None and pad_direction is not None) or remove_padding is True:\n",
        "            if remove_padding is False:\n",
        "                print(\"Warning: remove_padding is False, but pad_lengths and pad_direction were provided.\\n    Removing padding.\")\n",
        "                return True\n",
        "            if pad_lengths is None:\n",
        "                raise ValueError('pad_lengths must be provided as list of int values if remove_padding is True.')\n",
        "            if pad_direction is None:\n",
        "                raise ValueError('pad_direction \"left\" or \"right\" must be provided if remove_padding is True.')\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_padding(ids, pad_len: int, pad_direction: Literal[\"left\", \"right\"]):\n",
        "        if pad_len > 0:\n",
        "            if pad_direction == 'left':\n",
        "                pad_slice = slice(pad_len, None)\n",
        "            else:\n",
        "                pad_slice = slice(0, -pad_len)\n",
        "            ids = ids[pad_slice]\n",
        "        return ids\n",
        "\n",
        "    @classmethod\n",
        "    def batch_remove_padding(cls, batch_ids: List[torch.tensor] | torch.Tensor, pad_lengths: List[int], pad_direction: Literal[\"left\", \"right\"]):\n",
        "        assert len(batch_ids) == len(pad_lengths)\n",
        "        assert isinstance(batch_ids, torch.Tensor) or isinstance(batch_ids, list)\n",
        "        output_tensors = []\n",
        "        for i, pad_len in enumerate(pad_lengths):\n",
        "            output_tensors.append(cls.remove_padding(batch_ids[i], pad_len, pad_direction))\n",
        "        return output_tensors\n",
        "\n",
        "    @classmethod\n",
        "    def unbatch_hidden_state(cls, hidden_states, pad_lengths: Optional[List[int]], pad_direction: Optional[Literal[\"left\", \"right\"]]=None, remove_padding: bool=False):\n",
        "        unbatched_hidden_states = []\n",
        "        if remove_padding is True:\n",
        "            assert pad_lengths is not None\n",
        "            assert pad_direction is not None\n",
        "            unbatched_hidden_states.extend(cls.batch_remove_padding(batch_ids=hidden_states, pad_lengths=pad_lengths, pad_direction=pad_direction))\n",
        "        else:\n",
        "            unbatched_hidden_states.extend(list((hidden_states[i] for i in range(len(hidden_states)))))\n",
        "        return unbatched_hidden_states\n",
        "\n",
        "    @classmethod\n",
        "    def unbatch_hidden_states(cls, hidden_states, pad_lengths: Optional[List[int]]=None, pad_direction: Optional[Literal[\"left\", \"right\"]]=None, remove_padding: bool=False):\n",
        "        processed_states = {}\n",
        "        for tuple_idx, state_tuple in enumerate(hidden_states):\n",
        "            unbatched_hidden_states = LayersDict()\n",
        "            for layer_idx, layer_state in enumerate(state_tuple):\n",
        "                unbatched = cls.unbatch_hidden_state(hidden_states=layer_state, pad_lengths=pad_lengths, pad_direction=pad_direction, remove_padding=remove_padding)\n",
        "                unbatched_hidden_states[str(layer_idx)] = PromptDict(**{str(prompt_idx): prompt_state for prompt_idx, prompt_state in enumerate(unbatched)})\n",
        "            processed_states[tuple_idx] = unbatched_hidden_states\n",
        "        return processed_states\n",
        "\n",
        "    @classmethod\n",
        "    def print_text(cls, ids, tokenizer):\n",
        "        print(tokenizer.decode(ids))\n",
        "\n",
        "    @staticmethod\n",
        "    def batch_decode_text(batch_ids: torch.Tensor | List[torch.Tensor], tokenizer: AutoTokenizer):\n",
        "        batch_size = len(batch_ids)\n",
        "        decoded = []\n",
        "        for idx in range(0, batch_size):\n",
        "            ids = batch_ids[idx]\n",
        "            decoded.append(tokenizer.decode(ids))\n",
        "        return decoded\n",
        "\n",
        "    @classmethod\n",
        "    def print_batch_text(cls, batch_ids: torch.Tensor | List[torch.Tensor], tokenizer: AutoTokenizer, pad_lengths: Optional[List[int]]=None, pad_direction: Optional[Literal[\"left\", \"right\"]]=None, remove_padding: bool=False):\n",
        "        if cls._padding_args(pad_lengths=pad_lengths, pad_direction=pad_direction, remove_padding=remove_padding):\n",
        "            to_decode = cls.batch_remove_padding(batch_ids=batch_ids, pad_lengths=pad_lengths, pad_direction=pad_direction)\n",
        "        else:\n",
        "            to_decode = batch_ids\n",
        "        decoded = cls.batch_decode_text(batch_ids=to_decode, tokenizer=tokenizer)\n",
        "        for text in decoded:\n",
        "            print(text)\n",
        "\n",
        "    @staticmethod\n",
        "    def batch_slice_tokens(batch_ids, start_idxs, end_idxs):\n",
        "        \"\"\"\n",
        "        Slices the tensors across the sequence_length dimension based on start and end indexes.\n",
        "\n",
        "        Args:\n",
        "            batch_ids: A 3D tensor of shape (batch_size, sequence_length, activation),\n",
        "                or a Python list of 2D tensors of shape (sequence_length, activation) or\n",
        "                3D tensors of shape (1, sequence_length, activation).\n",
        "            start_idxs: A Python list of start indices for slicing.\n",
        "            end_idxs: A Python list of end indices for slicing.\n",
        "\n",
        "        Returns:\n",
        "            A list of sliced tensors for each batch.\n",
        "        \"\"\"\n",
        "        if isinstance(batch_ids, torch.Tensor) and batch_ids.ndim == 2:\n",
        "            batch_ids = batch_ids.unsqueeze(0)  # Convert 2D tensor to 3D\n",
        "\n",
        "        # If batch_ids is a 3D tensor, slice it directly\n",
        "        if isinstance(batch_ids, torch.Tensor) and batch_ids.ndim == 3:\n",
        "            batch_size = batch_ids.shape[0]\n",
        "            sliced_tensors = []\n",
        "            sequence_length = batch_ids.shape[1]\n",
        "            for i in range(batch_size):\n",
        "                check_start = start = start_idxs[i]\n",
        "                check_end = end = end_idxs[i]\n",
        "                if start is None:\n",
        "                    check_start = 0\n",
        "                if end is None:\n",
        "                    check_end = sequence_length\n",
        "                if check_start > sequence_length:\n",
        "                    raise IndexError(\"Start index exceeds sequence length.\")\n",
        "                if check_end > sequence_length:\n",
        "                    raise IndexError(\"End index exceeds sequence length.\")\n",
        "                if check_start > check_end:\n",
        "                    raise IndexError(\"Start index cannot be greater than end index.\")\n",
        "                sliced_tensors.append(batch_ids[i, start:end, :])  # Slicing the sequence_length dimension\n",
        "\n",
        "        # If batch_ids is a list of tensors\n",
        "        elif isinstance(batch_ids, list):\n",
        "            sliced_tensors = []\n",
        "            for i, tensor in enumerate(batch_ids):\n",
        "                start = start_idxs[i]\n",
        "                end = end_idxs[i]\n",
        "\n",
        "                # If the tensor is 3D, reduce it to 2D\n",
        "                if tensor.ndim == 3 and tensor.shape[0] == 1:\n",
        "                    tensor = tensor.squeeze(0)  # Convert (1, sequence_length, activation) -> (sequence_length, activation)\n",
        "\n",
        "                # Check for index errors\n",
        "                sequence_length = tensor.shape[0]\n",
        "                check_start = start = start_idxs[i]\n",
        "                check_end = end = end_idxs[i]\n",
        "                if start is None:\n",
        "                    check_start = 0\n",
        "                if end is None:\n",
        "                    check_end = sequence_length\n",
        "                if check_start > sequence_length:\n",
        "                    raise IndexError(\"Start index exceeds sequence length.\")\n",
        "                if check_end > sequence_length:\n",
        "                    raise IndexError(\"End index exceeds sequence length.\")\n",
        "                if check_start > check_end:\n",
        "                    raise IndexError(\"Start index cannot be greater than end index.\")\n",
        "                # Now slice the 2D tensor (sequence_length, activation)\n",
        "                sliced_tensors.append(tensor[start:end, :])\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Input data must be a 3D tensor or a list of 2D/3D tensors.\")\n",
        "\n",
        "        return sliced_tensors\n",
        "\n",
        "@torch.jit.script  # TorchScript for compilation\n",
        "def pca_using_svd(data: torch.Tensor, num_components: int = 1) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Perform PCA using SVD on the input data.\n",
        "\n",
        "    Args:\n",
        "        data (torch.Tensor): The input data of shape (n_samples, n_features).\n",
        "        num_components (int): The number of principal components to retain.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[torch.Tensor, torch.Tensor]: The principal components (right singular vectors),\n",
        "                                           and the explained variance (singular values squared).\n",
        "    \"\"\"\n",
        "    # Step 1: Center the data by subtracting the mean of each feature\n",
        "    mean = torch.mean(data, dim=0)\n",
        "    centered_data = data - mean\n",
        "\n",
        "    # Step 2: Perform SVD using torch.linalg.svd\n",
        "    # U: left singular vectors, S: singular values, Vh: right singular vectors (transposed)\n",
        "    U, S, Vh = torch.linalg.svd(centered_data, full_matrices=False)\n",
        "\n",
        "    # Step 3: Extract the top `num_components` principal components\n",
        "    Vh = Vh[:num_components, :]\n",
        "    S = S[:num_components]\n",
        "\n",
        "    # Step 4: Compute explained variance (which is the square of the singular values)\n",
        "    explained_variance = S ** 2 / (data.size(0) - 1)\n",
        "\n",
        "    # Return the principal components and the explained variance\n",
        "    return Vh, explained_variance\n",
        "\n",
        "@torch.jit.script  # TorchScript for compilation\n",
        "def rebatch_tokens_compiled(hidden_states: List[List[torch.Tensor]]) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Reshapes the data from the model's output to be in the desired format.\n",
        "\n",
        "    Args:\n",
        "        outputs (ModelOutputType): The input data, a dictionary where the hidden_states value is a tuple containing a nested tuple with a tensor for each layer.\n",
        "            The embedding info for prefill tokens is the tuple of layer tensors at element 0.\n",
        "            Tuples of hidden_states for each layer for each generated token make up the subsequent elements.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The tensors reshaped. (layer x batch_size x sequence x hidden_dimension).\n",
        "    \"\"\"\n",
        "    # 1. Extract the last token from each tensor in the hidden_states tuples (shape [10, 17 | 1, 2048] -> [10, 1, 2048])\n",
        "    # This is equivalent to taking the slice `[:, -1:, :]` for the last token\n",
        "    # Then we stack all the tensors along a new layer dimension (shape [22, 10, 1, 2048])\n",
        "    all_tensors = [\n",
        "        torch.stack([hs[:, -1:, :] for hs in hstuple], dim=0)\n",
        "        for hstuple in hidden_states\n",
        "    ] # shape list of lists[10, 1, 2048] for each layer  # shape [10, 1, 2048] for each layer\n",
        "\n",
        "    # 2. Stack tensors along a new dimension (num_layers, batch_size, sequence_length, hidden_dimension)\n",
        "    # We assume that each \"layer\" corresponds to each tensor we are stacking\n",
        "    # Resulting shape will be (num_layers, batch_size, sequence_length, hidden_dimension)\n",
        "    rebatched_hidden_states = torch.concat(all_tensors, dim=2)\n",
        "    return rebatched_hidden_states\n",
        "\n",
        "@torch.jit.script  # TorchScript for compilation\n",
        "def layer_wise_pca(rebatched_hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Performs layer-wise PCA on the hidden states.\n",
        "\n",
        "    Args:\n",
        "        rebatched_hidden_states (torch.Tensor): The rebatched model output data.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The identified principal components per layer. (layer x hidden_dimension x principal_components).\n",
        "    \"\"\"\n",
        "    layer, batch_size, sequence, hidden_dimension = rebatched_hidden_states.shape\n",
        "    rebatched_hidden_states = rebatched_hidden_states.view(layer, batch_size * sequence, hidden_dimension)\n",
        "    pca_result = []\n",
        "    for pca_tensor in rebatched_hidden_states.split(1):\n",
        "        pca_tensor = pca_tensor.squeeze(0)\n",
        "        # principal_components, explained_variance = pca_using_svd(pca_tensor, num_components=1)\n",
        "        principal_components, _ = pca_using_svd(pca_tensor, num_components=1)\n",
        "        pca_result.append(principal_components)\n",
        "    pca_result = torch.stack(pca_result, dim=0)\n",
        "    return pca_result\n",
        "\n",
        "class PaCEBuddy(LlmBuddy):\n",
        "    def __init__(self, model, tokenizer, device, gen_cfg, cache=None, db=None, db_path='/content/pace.db', backup_db_path=None):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device\n",
        "        self.gen_cfg = gen_cfg\n",
        "        if (not isinstance(cache, AsyncPaceCacheBuddy)) and cache is not None:\n",
        "            raise ArgumentError(\"cache must be of type AsyncPaceCacheBuddy or None\\nNot type: {type}\".format(type=type(cache)))\n",
        "        self.cache = cache\n",
        "        self.db_path = Path(db_path)\n",
        "        if db is None:\n",
        "            raise ArgumentError(\"db must be provided\")\n",
        "        self.db = db\n",
        "        self.backup_db_path = Path(backup_db_path)\n",
        "        self.pending_tasks = []\n",
        "        self.cache_tasks = []\n",
        "        self.backup_task = None\n",
        "        self.loop = asyncio.get_event_loop()\n",
        "        pass\n",
        "\n",
        "    def backup_db(self):\n",
        "        if self.backup_db_path is not None:\n",
        "            self.db.close()\n",
        "            buddy.file_pair(local_path=self.db_path, remote_path=self.backup_db_path).copy_to_remote()\n",
        "            self.db.connect()\n",
        "\n",
        "    def tokenize(self, prompts):\n",
        "        inputs = super().tokenize(prompts, self.tokenizer, self.device)\n",
        "        pad_unpad_lengths = [\n",
        "            tuple((\n",
        "                (ids == self.tokenizer.pad_token_id).sum().item(),\n",
        "                (ids != self.tokenizer.pad_token_id).sum().item()\n",
        "            )) for ids in inputs['input_ids']\n",
        "        ]\n",
        "\n",
        "        # Split the list of tuples into two lists\n",
        "        pad_lengths, unpad_lengths = zip(*pad_unpad_lengths)\n",
        "\n",
        "        # Convert the result back to lists (since zip returns tuples)\n",
        "        pad_lengths, unpad_lengths = list(pad_lengths), list(unpad_lengths)\n",
        "\n",
        "        return inputs, pad_lengths, unpad_lengths\n",
        "\n",
        "    def generate(self, inputs):\n",
        "        outputs = super().generate(inputs, self.model, self.gen_cfg)\n",
        "        return outputs\n",
        "\n",
        "    def remove_batch_padding(self, concepts, inputs, pad_lengths, unpad_lengths, outputs, pad_direction):\n",
        "        outputs = super().batch_remove_padding(batch_ids=outputs.sequences, pad_lengths=pad_lengths, pad_direction=pad_direction)\n",
        "        return concepts, inputs, pad_lengths, outputs\n",
        "\n",
        "    def rebatch_tokens(self, outputs):\n",
        "        \"\"\"\n",
        "        Reshapes the data from the model's output to be in the desired format.\n",
        "\n",
        "        Args:\n",
        "            outputs dict(str, tuple(tuple(torch.Tensor))): The input data, a dictionary where the hidden_states value is a tuple containing a nested tuple with a tensor for each layer.\n",
        "                The embedding info for prefill tokens is the tuple of layer tensors at element 0.\n",
        "                Tuples of hidden_states for each layer for each generated token make up the subsequent elements.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The tensors reshaped. (layer x batch_size x sequence x hidden_dimension).\n",
        "        \"\"\"\n",
        "        return rebatch_tokens_compiled(outputs.get(\"hidden_states\"))\n",
        "\n",
        "    def layer_wise_pca(self, rebatched_hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs layer-wise PCA on the hidden states.\n",
        "\n",
        "        Args:\n",
        "            rebatched_hidden_states (torch.Tensor): The rebatched model output data.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The identidied principal components per layer. (layer x hidden_dimension x principal_components).\n",
        "        \"\"\"\n",
        "        return layer_wise_pca(rebatched_hidden_states)\n",
        "\n",
        "\n",
        "    def print_text(self, concepts, inputs, pad_lengths, unpad_lengths, outputs):\n",
        "        decoded_texts = super().batch_decode_text(batch_ids=inputs.get(\"input_ids\"), tokenizer=self.tokenizer)\n",
        "        concept_text = {concept: [] for concept in concepts}\n",
        "        for concept, text in zip(concepts, decoded_texts):\n",
        "            concept_text[concept].append(text)\n",
        "        pprint(concept_text, indent=4)\n",
        "\n",
        "        if outputs is not None:\n",
        "            super().print_text(ids=outputs.sequences, tokenizer=self.tokenizer)\n",
        "\n",
        "    @staticmethod\n",
        "    def print_shape(tensor):\n",
        "        MatrixBuddy.print_shape(tensor)\n",
        "\n",
        "    async def update_cache(self, key, value):\n",
        "        task = self.loop.create_task(self.cache.insert_cache(key, value.clone().detach().cpu()))\n",
        "        self.cache_tasks.append(task)\n",
        "\n",
        "    async def get_cache(self, key):\n",
        "        rebatched_hidden_states = await self.cache.fetch_cache(key)\n",
        "        if rebatched_hidden_states is None:\n",
        "            raise ValueError(f\"Cache key {key} not found in cache despite being returned by list_keys method.\")\n",
        "        rebatched_hidden_states = rebatched_hidden_states.to(self.device)\n",
        "        return rebatched_hidden_states\n",
        "\n",
        "    async def insert_tensor(self, concepts, pca_result):\n",
        "        if self.backup_task is not None:\n",
        "            await self.backup_task\n",
        "            self.backup_task = None\n",
        "        task = self.loop.create_task(self.db.insert_tensor(concepts, pca_result.clone().detach().cpu()))\n",
        "        self.pending_tasks.append(task)\n",
        "\n",
        "    async def async_backup_db(self):\n",
        "            \"\"\"\n",
        "            Asynchronously performs an incremental backup of the database.\n",
        "            Uses SQLite's built-in backup feature to only transfer changed pages.\n",
        "            \"\"\"\n",
        "            # Complete any pending tasks before starting the backup\n",
        "            await asyncio.gather(*self.pending_tasks)\n",
        "            # Open the current database connection\n",
        "            async with aiosqlite.connect(self.db_path) as db:\n",
        "                # Ensure the backup path exists\n",
        "                if not self.backup_db_path.parent.exists():\n",
        "                    self.backup_db_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "                # Perform an incremental backup to the target backup database\n",
        "                async with aiosqlite.connect(self.backup_db_path) as backup_db:\n",
        "                    # SQLite's backup API transfers pages in steps\n",
        "                    await db.backup(backup_db, pages=1_000, sleep=0.1)\n",
        "                    print(f\"Incremental backup completed to {self.backup_db_path}\")\n",
        "                    # Close the backup database connection\n",
        "                    await backup_db.close()\n",
        "\n",
        "                print(f\"Backup completed to {self.backup_db_path}\")\n",
        "                # Close the current database connection\n",
        "                await db.close()\n",
        "\n",
        "    @staticmethod\n",
        "    def check_pca_input(hidden_states, layer_size, batch_size, sequence_length, hidden_dimension):\n",
        "            if len(hidden_states.shape) != 4:\n",
        "                raise ValueError(f\"Expected 4 dimensions, got {len(hidden_states.shape)}\")\n",
        "            if layer_size != hidden_states.shape[0]:\n",
        "                raise ValueError(f\"Expected {layer_size} layers, got {hidden_states.shape[0]}\")\n",
        "            if batch_size != hidden_states.shape[1]:\n",
        "                raise ValueError(f\"Expected {batch_size} batches, got {hidden_states.shape[1]}\")\n",
        "            if sequence_length != hidden_states.shape[2]:\n",
        "                raise ValueError(f\"Expected {sequence_length} sequence length, got {hidden_states.shape[2]}\")\n",
        "            if hidden_dimension != hidden_states.shape[3]:\n",
        "                raise ValueError(f\"Expected {hidden_dimension} hidden dimension, got {hidden_states.shape[3]}\")\n",
        "\n",
        "    async def main(self, dataset, dataset_len, batch_size=10, backup_interval=2, pad_direction=\"right\"):\n",
        "        uploaded_concepts = await self.db.list_concepts()\n",
        "        if self.cache is not None:\n",
        "            cached_activation_keys = await self.cache.list_keys()\n",
        "        else:\n",
        "            cached_activation_keys = []\n",
        "\n",
        "        max_new_tokens = self.gen_cfg.max_new_tokens\n",
        "        min_length = self.gen_cfg.min_length\n",
        "\n",
        "        layer_size = 22 + 1\n",
        "        sequence_length = max_new_tokens\n",
        "        hidden_dimension = 2048\n",
        "\n",
        "\n",
        "        progress_bar = tqdm(total=dataset_len, desc=\"Processing stimuli\")\n",
        "        generate_bar = tqdm(total=None, desc=f\"Generating in batches of {batch_size} from prompts\")\n",
        "        backup_description = \"Backing up database in {remaining} intervals.\"\n",
        "        backup_bar = tqdm(total=backup_interval, desc=backup_description.format(remaining=backup_interval))\n",
        "        # Create an event loop for handling async tasks\n",
        "        just_backed_up = False\n",
        "        for concept_idx, (iter_concept, iter_stimuli_batch) in enumerate(dataset):\n",
        "            batch_size = len(iter_stimuli_batch)\n",
        "            if iter_concept in uploaded_concepts:\n",
        "                continue\n",
        "            just_backed_up = False\n",
        "            hidden_states = []\n",
        "            generate_bar.reset(total=len(iter_stimuli_batch))\n",
        "\n",
        "            for iter_idx, iter_stimuli in enumerate(chunked(iter_stimuli_batch, n=batch_size)):\n",
        "                inputs, pad_lengths, unpad_lengths = self.tokenize(iter_stimuli)\n",
        "                cache_hit = False\n",
        "                if self.cache is not None:\n",
        "                    key = f\"{iter_concept} | {iter_idx}\"\n",
        "                    if key in cached_activation_keys:\n",
        "                        rebatched_hidden_states = await self.get_cache(key)\n",
        "                        pacelogger.info(f\"Cache hit for {key}\")\n",
        "                        cache_hit = True\n",
        "                if cache_hit is False:\n",
        "                    outputs = self.generate(inputs)\n",
        "                    rebatched_hidden_states = self.rebatch_tokens(outputs)\n",
        "                hidden_states.append(rebatched_hidden_states)\n",
        "                if self.cache is not None:\n",
        "                    await self.update_cache(key, rebatched_hidden_states)\n",
        "                generate_bar.update(len(iter_stimuli))\n",
        "\n",
        "            hidden_states = torch.concat(hidden_states, dim=1)  # concat along the batch dimension\n",
        "            progress_bar.set_description_str(f\"Performing PCA on concept {iter_concept} tensors\")\n",
        "            pacelogger.info(f\"Performing PCA on concept {iter_concept} tensors\\nTensor shape is {hidden_states.shape}\")\n",
        "            self.check_pca_input(hidden_states=hidden_states, layer_size=layer_size, batch_size=batch_size, sequence_length=sequence_length, hidden_dimension=hidden_dimension)\n",
        "            pca_result = self.layer_wise_pca(hidden_states)\n",
        "            paselogger.info(f\"PCA result shape is {pca_result.shape}\")\n",
        "            progress_bar.update(hidden_states.shape[1])\n",
        "\n",
        "            # Schedule async database insertion\n",
        "            await self.insert_tensor(iter_concept, pca_result)\n",
        "\n",
        "            backup_bar.update(1)\n",
        "            backup_bar.set_description_str(backup_description.format(remaining=backup_interval - (concept_idx % backup_interval)))\n",
        "            # Check if we need to backup\n",
        "            if concept_idx % backup_interval == 0:\n",
        "                # Wait for all pending tasks to complete, don't want to lose progress when colab shuts down\n",
        "                await asyncio.gather(*self.pending_tasks)\n",
        "                # Schedule async database backup\n",
        "                self.backup_task = self.loop.create_task(self.async_backup_db())\n",
        "                backup_bar.reset(total=backup_interval)\n",
        "                backup_bar.set_description_str(backup_description.format(remaining=backup_interval))\n",
        "                just_backed_up = True\n",
        "\n",
        "        # Perform final backup if needed\n",
        "        if not just_backed_up:\n",
        "            backup_task = self.loop.create_task(self.async_backup_db())\n",
        "            self.pending_tasks.append(backup_task)\n",
        "\n",
        "        # Wait for all pending tasks to complete\n",
        "        await asyncio.gather(*self.pending_tasks)\n",
        "\n",
        "        progress_bar.close()\n",
        "        generate_bar.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "niTXHm1ZPngP",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Concept Extraction Config\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "if \"concepts\" in locals():\n",
        "    old_concepts_len = len(concepts)\n",
        "full_concepts = get_concepts()\n",
        "concepts = full_concepts\n",
        "if \"old_concepts_len\" in locals():\n",
        "    concepts_changed = len(concepts) != old_concepts_len\n",
        "else:\n",
        "    concepts_changed = False\n",
        "\n",
        "print(concepts)\n",
        "\n",
        "# local_project_base = Path(\"/content/drive/MyDrive/Projects/Control_Vectors\")\n",
        "# remote_project_base = Path(\"/content\")\n",
        "dataset_len_json = buddy.file_pair(\n",
        "    local_path=local_project_base / \"dataset_len.json\",\n",
        "    remote_path=remote_project_base / \"dataset_len.json\"\n",
        ")\n",
        "\n",
        "\n",
        "if \"dataset_len\" not in locals() or concepts_changed is True:\n",
        "    if len(concepts) != len(full_concepts):\n",
        "        dataset_len = get_stimuli_len(concepts)\n",
        "    else:\n",
        "        if dataset_len_json.remote.exists():\n",
        "            dataset_len_json.copy_to_local()\n",
        "            with open(dataset_len_json.local, 'r') as f:\n",
        "                dataset_len = json.load(f)\n",
        "        else:\n",
        "            dataset_len = get_all_stimuli_len()\n",
        "            with open(dataset_len_json.local, 'w') as f:\n",
        "                json.dump(dataset_len, f)\n",
        "            dataset_len_json.copy_to_remote()\n",
        "\n",
        "db_filename = 'pace_{model_name}.db'.format(model_name=filesafe_model_name)\n",
        "db_path = buddy.file_pair(\n",
        "    local_path=local_project_base / db_filename,\n",
        "    remote_path=remote_project_base / db_filename\n",
        ")\n",
        "\n",
        "if db_path.remote.exists() is True:\n",
        "    db_path.copy_to_local()\n",
        "\n",
        "# db.fetch_tensor\n",
        "token_length = 5\n",
        "gen_cfg = GenerationConfig.from_model_config(model.config)\n",
        "gen_cfg.max_new_tokens = token_length\n",
        "gen_cfg.min_length = token_length\n",
        "gen_cfg.output_hidden_states = True\n",
        "gen_cfg.return_dict_in_generate = True\n",
        "\n",
        "cache_db_path = local_project_base / \"pace_cache.db\""
      ],
      "metadata": {
        "id": "1I22uf1sX5uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am still in heavy experimentation. sometimes I just need to delete everything and start over."
      ],
      "metadata": {
        "id": "2svz3Dl9mUfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(db_path.remote.exists())\n",
        "print(db_path.local.exists())\n",
        "print(cache_db_path.exists())\n",
        "# !rm {db_path.remote}\n",
        "# print(db_path.remote.exists())\n",
        "# !rm {db_path.local}\n",
        "# print(db_path.local.exists())\n",
        "# !rm {cache_db_path}\n",
        "# print(cache_db_path.exists())"
      ],
      "metadata": {
        "id": "jMF3kSbeBgiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "log_level = 1\n",
        "match log_level:\n",
        "    case 0:\n",
        "        logger.setLevel(logging.ERROR)\n",
        "    case 1:\n",
        "        logger.setLevel(logging.WARNING)\n",
        "    case 2:\n",
        "        logger.setLevel(logging.INFO)\n",
        "    case 3:\n",
        "        logger.setLevel(logging.DEBUG)\n",
        "    case _:\n",
        "        logger.setLevel(logging.ERROR)\n"
      ],
      "metadata": {
        "id": "hL8bLOS6ssZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This runs the implemented PAcE variant where I am trying to use algorithmic means of decomposing activations. I do not have the resources to effecitvely train sparse auto encoders at this time, so I am hoping to find a more traditionally mathematically grounded solution to representation engineering.\n",
        "\n",
        "I have encountered some interesting results thus far, but the model I am working with are small, and therefore very brittle and sensitive to interventions."
      ],
      "metadata": {
        "id": "ivc6NQAZmf2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Concept Extraction\n",
        "run_concept_extraction = False # @param {type:'boolean'}\n",
        "if run_concept_extraction:\n",
        "    model_prompt_batch_size = 10\n",
        "    # If None, full dataset is used\n",
        "    restrict_prompt_len = None\n",
        "    dataset = dataset_generator(concepts=concepts, load_n_concepts=3, restrict_stimuli=restrict_prompt_len)\n",
        "\n",
        "    db_flush_interval_minutes = 5\n",
        "    flush_interval = db_flush_interval_minutes * 60\n",
        "    db_batch_size = 100 if torch.cuda.is_available() else 1\n",
        "    db = await AsyncPaceDbBuddy.create_db(db_path=db_path.local, batch_size=db_batch_size, flush_interval=flush_interval)\n",
        "\n",
        "    cache_flush_interval = 60\n",
        "    cache_db = await AsyncPaceCacheBuddy.create_db(db_path=cache_db_path, batch_size=3, flush_interval=cache_flush_interval)\n",
        "\n",
        "    pace_buddy = PaCEBuddy(model=model, tokenizer=tokenizer, device=device, gen_cfg=gen_cfg, cache=cache_db, db=db, db_path=db_path.local, backup_db_path=db_path.remote)\n",
        "    await pace_buddy.main(dataset, dataset_len, batch_size=model_prompt_batch_size, backup_interval=db_batch_size, pad_direction=\"right\")"
      ],
      "metadata": {
        "id": "j-k0UeN-MzFp",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = PaceDbBuddy(db_path=db_path.local)\n",
        "stored_concepts = db.list_concepts()\n",
        "print(stored_concepts)"
      ],
      "metadata": {
        "id": "9WN9yiO-D2AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Functions for tensor operations\n",
        "import torch\n",
        "\n",
        "# @torch.jit.script  # TorchScript for compilation\n",
        "def soft_thresholding_inplace(coef: torch.Tensor, alpha: float):\n",
        "    \"\"\"In-place soft thresholding function for L1 regularization.\"\"\"\n",
        "    coef.abs_().sub_(alpha).clamp_(min=0.0).mul_(coef.sign_())\n",
        "    return coef\n",
        "\n",
        "# @torch.jit.script  # TorchScript for compilation\n",
        "def precompute_concept_vectors(concept_vectors: torch.Tensor, eps: float = 1e-5):\n",
        "    \"\"\"\n",
        "    Precompute necessary data for concept_vectors to optimize coordinate descent.\n",
        "    \"\"\"\n",
        "    # Normalize the concept vectors\n",
        "    concept_vectors = torch.nn.functional.normalize(concept_vectors, dim=1)\n",
        "\n",
        "    # Precompute norm squared for each concept vector, add eps to avoid divide by zero\n",
        "    norm_squared = torch.norm(concept_vectors, dim=1) ** 2 + eps  # (num_concepts,)\n",
        "\n",
        "    # Precompute the Gram matrix (concept_vectors.T @ concept_vectors) and add eps to the diagonal\n",
        "    gram_matrix = torch.matmul(concept_vectors, concept_vectors.T)  # (num_concepts x num_concepts)\n",
        "    gram_matrix = gram_matrix + torch.eye(gram_matrix.shape[0], device=gram_matrix.device) * eps\n",
        "\n",
        "    return concept_vectors, norm_squared, gram_matrix\n",
        "\n",
        "# @torch.jit.script  # TorchScript for compilation\n",
        "def coordinate_descent_batch_optimized(concept_vectors: torch.Tensor,\n",
        "                                       norm_squared: torch.Tensor,\n",
        "                                       gram_matrix: torch.Tensor,\n",
        "                                       latent_vector: torch.Tensor,\n",
        "                                       coefficients: torch.Tensor,\n",
        "                                       alpha: float,\n",
        "                                       l1_ratio: float,\n",
        "                                       tol: float,\n",
        "                                       max_iter: int,\n",
        "                                       clip_range: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Optimized coordinate descent for a batch of latent vectors, compiled with TorchScript.\n",
        "    \"\"\"\n",
        "    seq_len, hidden_dimension = latent_vector.shape\n",
        "    num_concepts = concept_vectors.shape[0]\n",
        "\n",
        "    for iteration in range(max_iter):\n",
        "        coefficients_old = coefficients.clone()\n",
        "\n",
        "        # Compute residuals for all latent vectors\n",
        "        residual = latent_vector - torch.matmul(coefficients, concept_vectors)\n",
        "\n",
        "        # Vectorized rho computation for non-converged concepts\n",
        "        rho = torch.sum(\n",
        "            concept_vectors.unsqueeze(0) * (residual.unsqueeze(1) + coefficients.unsqueeze(2) * concept_vectors.unsqueeze(0)),\n",
        "            dim=-1\n",
        "        )\n",
        "\n",
        "        # Perform soft-thresholding and coefficient updates in-place\n",
        "        coef_update = soft_thresholding_inplace(rho, alpha * l1_ratio)\n",
        "        coef_update.div_(gram_matrix.diag() + alpha * (1 - l1_ratio)).unsqueeze_(0)\n",
        "\n",
        "        # Store the coefficient changes for residual update\n",
        "        delta_coeff = coef_update - coefficients\n",
        "        coefficients = torch.clamp_(coef_update, clip_range[0], clip_range[1])\n",
        "\n",
        "        # Parallelized residual update using matrix multiplication\n",
        "        # Reshape concept_vectors.T to (num_concepts, 1, 1, hidden_dimension)\n",
        "        breakpoint()\n",
        "        residual -= torch.einsum('bsh,ch->bsh', delta_coeff, concept_vectors.T)\n",
        "\n",
        "        # Check convergence\n",
        "        coef_change = torch.norm(coefficients - coefficients_old, p=2)  # Compute change\n",
        "\n",
        "        if coef_change < tol:\n",
        "            break\n",
        "\n",
        "    return coefficients\n",
        "\n",
        "\n",
        "# @torch.jit.script  # TorchScript for compilation\n",
        "def decompose_latent_activation_cuda(latent_vector: torch.Tensor, concept_vectors: torch.Tensor,\n",
        "                                     norm_squared: torch.Tensor, gram_matrix: torch.Tensor,\n",
        "                                     alpha: float, l1_ratio: float, max_iter: int, tol: float,\n",
        "                                     clip_range: Tuple[float, float]=(-1e3, 1e3)):\n",
        "    \"\"\"\n",
        "    Decomposes a batch of latent vectors using CUDA streams, TorchScript, and AMP.\n",
        "    \"\"\"\n",
        "    clip_range = torch.tensor(clip_range, device=latent_vector.device)\n",
        "    batch_size, seq_len, hidden_dimension = latent_vector.shape\n",
        "    num_concepts = concept_vectors.shape[0]\n",
        "\n",
        "    coefficients = torch.zeros(batch_size, seq_len, num_concepts, device=latent_vector.device)\n",
        "\n",
        "    # AMP context for mixed precision training (automatic mixed precision)\n",
        "    with torch.cuda.amp.autocast(enabled=True):\n",
        "        streams = [torch.cuda.Stream() for _ in range(batch_size)]\n",
        "\n",
        "        for batch_idx in range(batch_size):\n",
        "            latent_batch = latent_vector[batch_idx]\n",
        "            with torch.cuda.stream(streams[batch_idx]):\n",
        "                coefficients[batch_idx] = coordinate_descent_batch_optimized(\n",
        "                    concept_vectors, norm_squared, gram_matrix, latent_batch, coefficients[batch_idx], alpha, l1_ratio, tol, max_iter, clip_range\n",
        "                )\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    return coefficients\n",
        "\n",
        "# @torch.jit.script  # TorchScript for compilation\n",
        "def decompose_latent_activation_cpu(latent_vector: torch.Tensor, concept_vectors: torch.Tensor,\n",
        "                                    norm_squared: torch.Tensor, gram_matrix: torch.Tensor,\n",
        "                                    alpha: float, l1_ratio: float, max_iter: int, tol: float,\n",
        "                                    clip_range: Tuple[float, float]=(-1e3, 1e3)):\n",
        "    \"\"\"\n",
        "    Decomposes a batch of latent vectors using CPU, TorchScript.\n",
        "    \"\"\"\n",
        "    clip_range = torch.tensor(clip_range, device=latent_vector.device)\n",
        "    batch_size, seq_len, hidden_dimension = latent_vector.shape\n",
        "    num_concepts = concept_vectors.shape[0]\n",
        "\n",
        "    coefficients = torch.zeros(batch_size, seq_len, num_concepts, device=latent_vector.device)\n",
        "\n",
        "    for batch_idx in range(batch_size):\n",
        "        latent_batch = latent_vector[batch_idx]\n",
        "        coefficients[batch_idx] = coordinate_descent_batch_optimized(\n",
        "            concept_vectors, norm_squared, gram_matrix, latent_batch, coefficients[batch_idx], alpha, l1_ratio, tol, max_iter, clip_range\n",
        "        )\n",
        "\n",
        "    return coefficients\n",
        "\n",
        "\n",
        "# Global variable to store the selected decomposition function\n",
        "# decompose_latent_activation = None\n",
        "\n",
        "def initialize_decomposition_fn():\n",
        "    \"\"\"\n",
        "    Initialize the appropriate decomposition function (either CUDA or CPU) based on CUDA availability.\n",
        "    This function will be called once during startup.\n",
        "    \"\"\"\n",
        "    global decompose_latent_activation\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        decompose_latent_activation = decompose_latent_activation_cuda\n",
        "    else:\n",
        "        decompose_latent_activation = decompose_latent_activation_cpu\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    decompose_latent_activation = decompose_latent_activation_cuda\n",
        "else:\n",
        "    decompose_latent_activation = decompose_latent_activation_cpu"
      ],
      "metadata": {
        "id": "c0MJuF1S2Tzm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.jit.script  # TorchScript for compilation\n",
        "def edit_latent_vector(coefficients: torch.Tensor, concept_vectors: torch.Tensor, concept_strengths: torch.Tensor, residual: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Edits the latent vector by scaling the contributions of specified concept directions.\n",
        "\n",
        "    Parameters:\n",
        "        coefficients (torch.Tensor): The coefficients for each concept.\n",
        "        concept_vectors (torch.Tensor): The matrix of concept vectors (dim x concept).\n",
        "        concept_strengths (torch.Tensor): Array of scale factors where the index of the strength value matches the index of the concept vector.\n",
        "        residual (torch.Tensor): The residual part of the latent vector.\n",
        "\n",
        "    Returns:\n",
        "        edited_latent_vector (torch.Tensor): The modified latent vector.\n",
        "    \"\"\"\n",
        "    # Scale coefficients by concept strengths using element-wise multiplication\n",
        "    scaled_coefficients = coefficients * concept_strengths\n",
        "\n",
        "    # Reconstruct the latent vector with scaled coefficients\n",
        "    # Expand concept_vectors to match the dimensions of scaled_coefficients\n",
        "    # concept_vectors has shape [hidden_dimension, num_concepts]\n",
        "    # scaled_coefficients has shape [batch_size, seq_len, hidden_dimension]\n",
        "    # residual has shape [batch_size, seq_len, hidden_dimension]\n",
        "    # We'll transpose it to [num_concepts, hidden_dimension] and then add batch/seq_len dimensions\n",
        "    concept_vectors = concept_vectors.T.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, num_concepts, hidden_dimension]\n",
        "\n",
        "    # Multiply scaled_coefficients with concept_vectors and sum across the concepts dimension (dim=2)\n",
        "    edited_latent_vector = torch.sum(scaled_coefficients.unsqueeze(-1) * concept_vectors, dim=2) + residual\n",
        "\n",
        "    return edited_latent_vector"
      ],
      "metadata": {
        "id": "5QNva3WK2WTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "@torch.jit.script  # TorchScript for compilation\n",
        "def apply_scaling(latent_tensor: torch.Tensor, scaling_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    # Reshape scaling_tensor to (1, 1, z) for broadcasting\n",
        "    scaling_tensor_reshaped = scaling_tensor.view(1, 1, scaling_tensor.shape[0])\n",
        "    # Apply the scaling by broadcasting the scaling_tensor across the (x, y) dimensions\n",
        "    output = latent_tensor * scaling_tensor_reshaped\n",
        "    return output"
      ],
      "metadata": {
        "id": "oLRUYg6n4Yki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.jit.script\n",
        "def manipulate_activations(activations: torch.Tensor, pca_matrix: torch.Tensor, scaling_factors: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Manipulate activations by either broadcasting or matrix multiplication, depending on the size of pca_matrix.\n",
        "\n",
        "    Args:\n",
        "    - activations (Tensor): The activations from the model with shape (batch_size, sequence_length, hidden_dimension).\n",
        "    - pca_matrix (Tensor): The PCA matrix with shape (1, hidden_dimension) or (num_pca_components, hidden_dimension).\n",
        "    - scaling_factors (Tensor): A tensor of shape (num_pca_components,) or scalar for broadcasting.\n",
        "\n",
        "    Returns:\n",
        "    - modified_activations (Tensor): The manipulated activations with shape (batch_size, sequence_length, hidden_dimension).\n",
        "    \"\"\"\n",
        "    batch_size, sequence_length, hidden_dimension = activations.size()\n",
        "\n",
        "    # Case 1: Single PCA vector (e.g., shape (1, hidden_dimension)), broadcasting\n",
        "    if pca_matrix.size(0) == 1:\n",
        "        if scaling_factors.dim() == 0:\n",
        "            scaling_factors = scaling_factors.unsqueeze(0)  # Convert scalar to tensor for broadcasting\n",
        "\n",
        "        # Scale the PCA vector\n",
        "        scaled_pca_vector = pca_matrix * scaling_factors  # (1, hidden_dimension)\n",
        "\n",
        "        # Expand the PCA vector to match (batch_size, sequence_length, hidden_dimension)\n",
        "        scaled_pca_vector = scaled_pca_vector.expand(batch_size, sequence_length, hidden_dimension)\n",
        "\n",
        "        # Apply the modification by adding the scaled PCA vector\n",
        "        modified_activations = activations + scaled_pca_vector\n",
        "\n",
        "    # Case 2: Multiple PCA components\n",
        "    else:\n",
        "        # Reshape activations for matrix multiplication\n",
        "        activations_flat = activations.view(batch_size * sequence_length, hidden_dimension)\n",
        "\n",
        "        # Project activations onto the PCA components\n",
        "        pca_projected = torch.matmul(activations_flat, pca_matrix.t())  # (batch_size * sequence_length, num_pca_components)\n",
        "\n",
        "        # Scale the projected activations\n",
        "        pca_projected = pca_projected * scaling_factors\n",
        "\n",
        "        # Reconstruct the activations in the original space\n",
        "        modified_activations_flat = torch.matmul(pca_projected, pca_matrix)  # (batch_size * sequence_length, hidden_dimension)\n",
        "\n",
        "        # Reshape back to (batch_size, sequence_length, hidden_dimension)\n",
        "        modified_activations = modified_activations_flat.view(batch_size, sequence_length, hidden_dimension)\n",
        "\n",
        "    return modified_activations\n",
        "\n",
        "def get_range(tensor: torch.Tensor):\n",
        "    min_value = torch.min(tensor)\n",
        "    max_value = torch.max(tensor)\n",
        "    print('{')\n",
        "    print(f'\"min\": {min_value},')\n",
        "    print(f'\"max\": {max_value}')\n",
        "    print('},')"
      ],
      "metadata": {
        "id": "RYMdoi7NBPH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from typing import List\n",
        "\n",
        "class PaCEIntervention:\n",
        "    def __init__(self, model: torch.nn.Module, db: 'PaceDbBuddy'):\n",
        "        \"\"\"\n",
        "        Initialize the PaCE Intervention class.\n",
        "\n",
        "        Args:\n",
        "        - model: The pretrained language model (e.g., AutoModelForCausalLM).\n",
        "        - db: The embedding database with concept vectors.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.db = db\n",
        "        self.hooks = []\n",
        "        self.buffers = []\n",
        "        self.intervened_activations = {}\n",
        "        self.concept_tensor = None\n",
        "        self.strengths = None\n",
        "\n",
        "    def __del__(self):\n",
        "        self.remove_hooks()\n",
        "        del self.model\n",
        "        del self.db\n",
        "        del self.hooks\n",
        "        del self.buffers\n",
        "        del self.intervened_activations\n",
        "        del self.concept_tensor\n",
        "        del self.strengths\n",
        "\n",
        "    def enhance_concepts(self, concepts: List[str], strengths: List[float]):\n",
        "        concept_tensors = []\n",
        "        for concept, strength in zip(concepts, strengths):\n",
        "            tensor = self.db.fetch_tensor(concept)\n",
        "            if tensor is None:\n",
        "                raise ValueError(\"Concept {concept} not found in the database.\".format(concept=concept))\n",
        "            tensor = tensor[1:, : , : ] # Remove debug data from tensor\n",
        "            # tensor should be shape (layers, hidden_dimension, 1)\n",
        "            concept_tensors.append(tensor)\n",
        "        combined_concept_tensor = torch.concat(concept_tensors, dim = -1) # Stacks all principal components into a tensor of shape (hidden_dimension, num_components)\n",
        "        # combined_concept_tensor should be shape (layers, hidden_dimension, 1)\n",
        "        self.concept_tensor = combined_concept_tensor\n",
        "        strengths_tensor = torch.tensor(strengths, dtype=torch.float32).unsqueeze(0)\n",
        "        self.strengths = strengths_tensor\n",
        "\n",
        "    def apply_hook(self, layer_strength=True, first_layer_only=False):\n",
        "        if self.concept_tensor is None:\n",
        "            raise AttributeError(\"Concept tensor not set. Call enhance_concepts first.\")\n",
        "        concept_vectors = self.concept_tensor\n",
        "        strengths = self.strengths\n",
        "        layer_strength = []\n",
        "        for layer_idx, (concept_vector, strength) in enumerate(zip(concept_vectors.split(1, dim=0), strengths.split(1, dim=0))):\n",
        "            if first_layer_only is True and layer_idx != 0:\n",
        "                break\n",
        "            concept_vector = concept_vector.squeeze(0) # Remove layer dimension, should now be (hidden_dimension, 1)\n",
        "            if layer_strength:\n",
        "                strength = strength * torch.tensor((layer_idx / len(self.model.model.layers))).unsqueeze(0)  # Gradual increase\n",
        "\n",
        "            # Register the hook on the specified layer\n",
        "            buffer_concept = f\"concept_vector_layer_{layer_idx}\"\n",
        "            buffer_strength = f\"strength_vector_layer_{layer_idx}\"\n",
        "            layer_module = self.model.model.layers[layer_idx]\n",
        "            layer_module.register_buffer(name=buffer_concept, tensor=concept_vector, persistent=False)\n",
        "            layer_module.register_buffer(name=buffer_strength, tensor=strength, persistent=False)\n",
        "            self.buffers.append((layer_module, buffer_concept, buffer_strength))\n",
        "\n",
        "            hook_fn = self.create_hook_fn(concept_vector, buffer_concept, buffer_strength)\n",
        "            handle = layer_module.register_forward_hook(hook_fn)\n",
        "            self.hooks.append(handle)\n",
        "\n",
        "    def create_hook_fn(self, concept_vector, buffer_concept, buffer_strength):\n",
        "        \"\"\"\n",
        "        Create a hook function that modifies the output of the layer using precomputed concept vectors.\n",
        "        \"\"\"\n",
        "\n",
        "        def hook_fn(module, input: Tuple[torch.Tensor], output: Tuple[torch.Tensor]) -> Tuple[torch.Tensor]:\n",
        "            \"\"\"\n",
        "            A method that modifies the layer output by scaling the latent vectors based on precomputed tensors.\n",
        "\n",
        "            Parameters:\n",
        "            - module: The layer where the hook is registered.\n",
        "            - input: Input to the layer.\n",
        "            - output: Output from the layer (latent activation).\n",
        "\n",
        "            Returns:\n",
        "            - Modified output with the scaled latent vectors.\n",
        "            \"\"\"\n",
        "            strength = getattr(module, buffer_strength) # Shape (1, 1) unsqueezed scalar value turned into a tensor\n",
        "            concept = getattr(module, buffer_concept).T # Shape (1, n_features) for single component (x, n_features) for multiple components\n",
        "            latent_vector = output[0].clone().detach()\n",
        "            edited_latent_vector = manipulate_activations(latent_vector, concept, strength)\n",
        "            # modified_latent_vector = torch.clamp(edited_latent_vector, min=-0.55, max=0.55)  # Clip values\n",
        "\n",
        "            # Return the modified output\n",
        "            return tuple((edited_latent_vector, *output[1:]))\n",
        "\n",
        "        return hook_fn\n",
        "\n",
        "    def remove_hooks(self):\n",
        "        \"\"\"Remove all hooks from the model.\"\"\"\n",
        "        for handle in self.hooks:\n",
        "            handle.remove()\n",
        "        self.hooks.clear()\n",
        "        for layer_module, buffer_concept, buffer_strength in self.buffers:\n",
        "            try:\n",
        "                delattr(layer_module, buffer_concept)\n",
        "            except AttributeError:\n",
        "                # Handle the case where the attribute doesn't exist\n",
        "                pass\n",
        "            try:\n",
        "                delattr(layer_module, buffer_strength)\n",
        "            except AttributeError:\n",
        "                # Handle the case where the attribute doesn't exist\n",
        "                pass\n",
        "        self.buffers = []\n",
        "        self.concept_tensor = None\n",
        "        self.strengths = None"
      ],
      "metadata": {
        "id": "lO80UbhI0ZOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_tokens = 20\n",
        "\n",
        "gen_cfg_inf = GenerationConfig.from_model_config(model.config)\n",
        "gen_cfg_inf.max_new_tokens = new_tokens\n",
        "gen_cfg_inf.min_length = new_tokens\n",
        "gen_cfg_inf.output_hidden_states = False\n",
        "\n",
        "# Prepare inputs (token IDs)\n",
        "# inputs = tokenizer(\"Once upon a time\", return_tensors=\"pt\").to(model.device)\n",
        "# prompt = \"Once upon a time\"\n",
        "prompt = \"User: How old are you?\\nAssistant: I am \"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "if \"activation_wrangler\" in locals():\n",
        "    activation_wrangler.remove_hooks()\n",
        "    del activation_wrangler\n",
        "\n",
        "if \"unenhanced_generated_text\" not in locals() or locals().get(\"old_prompt\", \"\") != prompt:\n",
        "    old_prompt = prompt\n",
        "    output = model.generate(\n",
        "        input_ids=inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        generation_config=gen_cfg_inf\n",
        "    )\n",
        "    # Print generated text\n",
        "    unenhanced_generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"unenhanced\")\n",
        "print(unenhanced_generated_text)\n",
        "\n",
        "activation_wrangler = PaCEIntervention(model, db)\n",
        "concepts_strengths = {\n",
        "    # 'Alabama': 1.2,\n",
        "    '16-year-old': 0.17 # 1.357e-05\n",
        "}\n",
        "concepts = list(concepts_strengths.keys())\n",
        "strengths = list(concepts_strengths.values())\n",
        "activation_wrangler.enhance_concepts(concepts=concepts, strengths=strengths)\n",
        "\n",
        "print('concepts')\n",
        "print(concepts)\n",
        "print('strengths')\n",
        "print(strengths)\n",
        "\n",
        "activation_wrangler.apply_hook()\n",
        "\n",
        "# Perform the intervention\n",
        "enhanced_output = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    generation_config=gen_cfg_inf\n",
        ")\n",
        "enhanced_generated_text = tokenizer.decode(enhanced_output[0], skip_special_tokens=True)\n",
        "print(\"enhanced\")\n",
        "print(enhanced_generated_text)"
      ],
      "metadata": {
        "id": "fTPxG6EzFfGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concepts = [\n",
        "    '16-year-old',\n",
        "    \"'90s\"\n",
        "]\n",
        "for concept in concepts:\n",
        "    data = db.fetch_tensor(concept)\n",
        "    if data is not None:\n",
        "        print(data.shape)\n",
        "    else:\n",
        "        print(f\"Concept '{concept}' not found in the database.\")"
      ],
      "metadata": {
        "id": "kw7motGlbaRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Test suite for testing the representation engineering interventions at different strengths, and for different concepts.\n",
        "\n",
        "When using Tiny Llama 1B as a target model, with the intervention scaling decaying over each layer, I was able to notice a trend of the model's output age to become younger as the concept of '16-year-old' (I only picked this concept from the dataset as it seemed easiest to test, I.E. How old are you?)\n",
        "\n",
        "Without intervention the model stated it was roughly 24, as the intervention strength increased the model's age began to lower to 18, but soon the model collapsed into gibberish and was unable to properly continue with next token prediction.\n",
        "\n",
        "A larger model like Phi 3B may prove to be less brittle."
      ],
      "metadata": {
        "id": "ummkuAYHnoiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def test_strength(prompt, concepts, strengths, new_tokens=5):\n",
        "    gen_cfg_inf = GenerationConfig.from_model_config(model.config)\n",
        "    gen_cfg_inf.max_new_tokens = new_tokens\n",
        "    gen_cfg_inf.min_length = new_tokens\n",
        "    gen_cfg_inf.output_hidden_states = False\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    activation_wrangler = PaCEIntervention(model, db)\n",
        "    activation_wrangler.enhance_concepts(concepts=concepts, strengths=strengths)\n",
        "    activation_wrangler.apply_hook()\n",
        "    enhanced_output = model.generate(\n",
        "        input_ids=inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        generation_config=gen_cfg_inf\n",
        "    )\n",
        "    enhanced_generated_text = tokenizer.decode(enhanced_output[0], skip_special_tokens=True)\n",
        "    activation_wrangler.remove_hooks()\n",
        "    del activation_wrangler\n",
        "    return strengths, enhanced_generated_text\n",
        "\n",
        "\n",
        "new_tokens = 5\n",
        "\n",
        "concepts_prompts = {\n",
        "    '16-year-old': \"My birthday is next week, I'm turning \",\n",
        "    \"'90s\": \"The year is \"\n",
        "}\n",
        "for test_idx, (concept, prompt) in enumerate(concepts_prompts.items()):\n",
        "    print(f\"# Test {test_idx}\")\n",
        "    print(f\"New Tokens to generate: {new_tokens}\")\n",
        "    print(f\"Concept: {concept}\")\n",
        "    print('---')\n",
        "    for i in range(0, 25, 1):\n",
        "        strength = i / 100\n",
        "        strengths = [strength] * len(concepts)\n",
        "        strength, text = test_strength(prompt, concepts, strengths, new_tokens)\n",
        "        print(f\"Strength: {strength}\\nOutput: {text}\")\n",
        "        print('---')"
      ],
      "metadata": {
        "id": "QONUK3A8VZxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "YPY7vr3mARvI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}