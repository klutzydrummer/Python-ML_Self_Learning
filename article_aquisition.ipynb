{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPYrKbbvSuLLFeNYudZqe5o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/klutzydrummer/Python_Projects/blob/main/article_aquisition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import json\n",
        "with open(\"/content/drive/MyDrive/Machine_Learning_Digestor/config/connection_string.json\", \"r\") as f:\n",
        "    connection_string = json.load(f)\n",
        "\n",
        "search_date = \"07/10/2023\"\n",
        "search_period = {\"days\": 0, \"weeks\": 0, \"months\": 0, \"years\": 10}\n",
        "stock_symbol = \"MSFT\"\n",
        "index_name = \"test-index\"\n",
        "\n",
        "embeddings_model_name = \"hkunlp/instructor-large\"\n",
        "\n",
        "model_choice = \"stablebeluga-7b.ggmlv3.q4_1.bin\"\n",
        "model_context_window = 2048\n",
        "\n",
        "embedding_context_window = 512\n",
        "\n",
        "\n",
        "enable_debugprint = False\n",
        "from pathlib import Path as path\n",
        "\n",
        "path_drive_base = path(\"/content/drive/MyDrive/Machine_Learning_Digestor/\")\n",
        "path_local_base = path(\"/content/Machine_Learning_Digestor/\")\n",
        "drive_model_path = path_drive_base.joinpath(\"models\").joinpath(model_choice)\n",
        "local_cache_file = path(\"/content/Machine_Learning_Digestor/lanchain-cache.db\")\n",
        "local_cache_file.parent.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=False)\n",
        "\n",
        "def convert_to_drive_path(local_file_path: path) -> path:\n",
        "    if str(path_local_base) not in str(local_file_path):\n",
        "        raise ValueError(f\"Source path not in Local path:\\nSource: {local_file_path}\\nLocal path: {path_local_base}\")\n",
        "    result = str(local_file_path).replace(str(path_local_base), str(path_drive_base))\n",
        "    return path(result)\n",
        "\n",
        "def convert_to_local_path(drive_file_path: path) -> path:\n",
        "    if str(path_drive_base) not in str(drive_file_path):\n",
        "        raise ValueError(f\"Source path not in GDrive path:\\nSource: {drive_file_path}\\nGDrive path: {path_drive_base}\")\n",
        "    result = str(drive_file_path).replace(str(path_drive_base), str(path_local_base))\n",
        "    return path(result)\n",
        "\n",
        "async def download_from_drive(drive_file_path: path) -> path:\n",
        "    local_file_path = convert_to_local_path(drive_file_path)\n",
        "    local_file_path.parent.mkdir(exist_ok=True, parents=True)\n",
        "    shutil.copy(drive_file_path, local_file_path)\n",
        "    return local_file_path\n",
        "\n",
        "async def upload_to_drive(local_file_path: path, overwrite=False) -> path:\n",
        "    drive_file_path = convert_to_drive_path(local_file_path)\n",
        "    drive_file_path.parent.mkdir(exist_ok=True, parents=True)\n",
        "    if overwrite is True:\n",
        "        shutil.copy(local_file_path, drive_file_path)\n",
        "    else:\n",
        "        if drive_file_path.exists() is True:\n",
        "            raise ValueError(\"File already exists at destination and overwrite is not set to True.\")\n",
        "        else:\n",
        "            shutil.copy(local_file_path, drive_file_path)\n",
        "    return drive_file_path\n",
        "\n",
        "async def path_future(file_path: path) -> path:\n",
        "    return file_path\n",
        "\n",
        "local_model_path = convert_to_local_path(drive_model_path)\n",
        "\n",
        "pyproject_path = path(\"/content/pyproject.toml\")\n",
        "pyrequirements_path = path(\"/content/requirements.txt\")\n",
        "\n",
        "if local_cache_file.exists() is not True:\n",
        "    drive_cache_file = convert_to_drive_path(local_cache_file)\n",
        "    if drive_cache_file.exists() is True:\n",
        "        cache_file = download_from_drive(drive_cache_file)\n",
        "    else:\n",
        "        cache_file = path_future(local_cache_file)\n",
        "else:\n",
        "    cache_file = path_future(local_cache_file)\n",
        "\n",
        "if local_model_path.exists() is not True:\n",
        "    local_model = download_from_drive(drive_model_path)\n",
        "\n",
        "if pyrequirements_path.exists() is not True:\n",
        "    with open(pyrequirements_path, \"w\") as project_file:\n",
        "        project_file.write('''numpy\n",
        "    asyncpg\n",
        "    nltk\n",
        "\n",
        "    pyyaml >= 6.0.1\n",
        "    tqdm >= 4.66.0\n",
        "    googlenews >= 1.6.8\n",
        "    newspaper3k >= 0.2.8\n",
        "    langchain >= 0.0.260\n",
        "    sentence-transformers >= 2.2.2\n",
        "    InstructorEmbedding >= 1.0.1\n",
        "    asyncio >= 3.4.3\n",
        "    lmql >= 0.0.6.6\n",
        "    llama-cpp-python >= 0.1.77\n",
        "    pendulum >= 2.1.2\n",
        "    pandas >= 2.0.3\n",
        "    torch >= 2.0.0\n",
        "    psycopg2-binary >= 2.9.7''')\n",
        "\n",
        "    !cd /content; pip install -r \"/content/requirements.txt\"\n",
        "    # %pip install torch==2.0.0\n",
        "    # %pip uninstall -y numpy\n",
        "    # %pip install numpy"
      ],
      "metadata": {
        "id": "UWX-IfKDNPKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from GoogleNews import GoogleNews\n",
        "from newspaper import Article\n",
        "from pydantic import BaseModel, Field, validator\n",
        "from tqdm import tqdm\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "from typing import List, Optional\n",
        "import aiohttp\n",
        "import asyncio\n",
        "import dataclasses\n",
        "import datetime\n",
        "import itertools\n",
        "import json\n",
        "import pandas as pd\n",
        "import pendulum\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import functools\n",
        "import hashlib\n",
        "import asyncpg"
      ],
      "metadata": {
        "id": "26NfXV7EDMZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import PGEmbedding\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.cache import SQLiteCache\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "from langchain.vectorstores.base import VectorStore\n",
        "\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.llms import LlamaCpp\n",
        "import langchain\n",
        "try:\n",
        "    await cache_file\n",
        "except:\n",
        "    pass\n",
        "\n",
        "langchain.llm_cache = SQLiteCache(database_path=str(local_cache_file))"
      ],
      "metadata": {
        "id": "jUdI1ZYSvhvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"Preprocess the text.\"\"\"\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    print(tokens)\n",
        "\n",
        "    # Lowercasing\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    print(tokens)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    print(tokens)\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "bstPQxFLntX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "def str_to_uuid(in_string: str) -> uuid.UUID:\n",
        "    hash_machine = hashlib.md5()\n",
        "    hash_machine.update(bytes(in_string, \"ascii\"))\n",
        "    hash_bytes = int(hash_machine.hexdigest(), 16)\n",
        "    return uuid.UUID(int=hash_bytes)\n",
        "\n",
        "def custom_uuid(stock_symbol, title):\n",
        "    return str(str_to_uuid(f\"{stock_symbol} {title}\"))"
      ],
      "metadata": {
        "id": "QQiXbZApu-mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def load_embeddings():\n",
        "    global embeddings_instance\n",
        "    try:\n",
        "        return embeddings_instance\n",
        "    except Exception as err:\n",
        "        embeddings_instance = HuggingFaceInstructEmbeddings(\n",
        "            model_name=embeddings_model_name,\n",
        "            embed_instruction=\"Represent this financial article for clustering: \")\n",
        "        return embeddings_instance\n",
        "\n",
        "embeddings = load_embeddings\n",
        "_ = embeddings()"
      ],
      "metadata": {
        "id": "3PpwZ_d0v2pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def load_llm():\n",
        "    global llm_instance\n",
        "    try:\n",
        "        return llm_instance\n",
        "    except Exception as err:\n",
        "        try:\n",
        "            model_path = str(await local_model)\n",
        "        except Exception as err:\n",
        "            model_path = str(local_model_path)\n",
        "        llm_instance = LlamaCpp(\n",
        "            model_path=model_path,\n",
        "            n_ctx=model_context_window,\n",
        "            # input={\"temperature\": 0.75, \"max_length\": 2000, \"top_p\": 1},\n",
        "            # callback_manager=callback_manager,\n",
        "            verbose=True\n",
        "        )\n",
        "        return llm_instance\n",
        "\n",
        "summary_engine = load_llm\n",
        "_ = summary_engine()"
      ],
      "metadata": {
        "id": "UShKUDvDA-fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_time_to_str(datetime: datetime.datetime):\n",
        "    return f\"{int(datetime.month):02}/{int(datetime.day):02}/{int(datetime.year)}\""
      ],
      "metadata": {
        "id": "KQovTvkZfE-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if type(search_date) is str:\n",
        "    if search_date != \"\":\n",
        "        search_date = pendulum.from_format(search_date, 'MM/DD/YYYY')\n",
        "    else:\n",
        "        raise ValueError(\"Search date required.\")\n",
        "\n",
        "if type(search_date) is pendulum.DateTime:\n",
        "    end_date = custom_time_to_str(search_date)\n",
        "    start_date = custom_time_to_str(search_date.subtract(**search_period))\n",
        "    print(f\"Searching for articles released between {start_date} and {end_date}\")\n",
        "    googlenews = GoogleNews(lang=\"en\", encode=\"utf-8\", start=start_date, end=end_date)\n",
        "elif type(search_date) is str and search_date == \"\":\n",
        "    print(\"Searching for articles released anytime.\")\n",
        "    googlenews = GoogleNews(lang=\"en\", encode=\"utf-8\")\n",
        "else:\n",
        "    raise ValueError(f\"Type of search_date not valid, type is: {type(search_date)}\\nvalue is: {search_date}\")\n",
        "googlenews.clear()"
      ],
      "metadata": {
        "id": "JyAfukaTDosB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "googlenews.enableException(True)\n",
        "\n",
        "search_string = f\"stock news {stock_symbol}\"\n",
        "print(f\"Creating search for: {search_string}\")\n",
        "googlenews = GoogleNews(lang=\"en\")\n",
        "googlenews.get_news(search_string)\n",
        "news_list = googlenews.get_links()\n",
        "print(news_list)\n"
      ],
      "metadata": {
        "id": "zVjNg746Ds_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclasses.dataclass(init=True, repr=True, eq=True)\n",
        "class InputArticleRaw:\n",
        "    embeddable: str\n",
        "    title: str\n",
        "    text: str\n",
        "    authors: list[str]\n",
        "    publish_date: datetime.datetime\n",
        "    search_date: datetime.datetime\n",
        "    tags: set[str]\n",
        "    keywords: set[str]\n",
        "    stock_symbol: str\n",
        "\n",
        "\n",
        "    def __ini__(self, title: str, text: str, authors: list[str], publish_date: pendulum.DateTime, search_date: pendulum.DateTime, tags: set[str], keywords: set[str], stock_symbol: str, collection_id=None, embeddable=\"\"):\n",
        "        self.embeddable = \"\"\n",
        "        self.title = title\n",
        "        self.text = text\n",
        "        self.authors = authors\n",
        "        self.publish_date = publish_date\n",
        "        self.search_date = search_date\n",
        "        self.tags = tags\n",
        "        self.keywords = keywords\n",
        "        self.stock_symbol = stock_symbol\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def import_newspaper(newspaper_article: Article, stock_symbol: str):\n",
        "        if type(newspaper_article.publish_date) is not datetime.datetime:\n",
        "            raise ValueError(f\"Publish date is not of type datetime:\\n type:\\n {type(newspaper_article.publish_date)}\\n value:\\n {newspaper_article.publish_date}\")\n",
        "        result = InputArticleRaw(\n",
        "            embeddable=\"\",\n",
        "            title=newspaper_article.title,\n",
        "            text=newspaper_article.text,\n",
        "            authors=newspaper_article.authors,\n",
        "            publish_date=datetime.datetime.fromtimestamp(newspaper_article.publish_date.timestamp()),\n",
        "            search_date=datetime.datetime.fromtimestamp(search_date.timestamp()),\n",
        "            tags=newspaper_article.tags,\n",
        "            keywords=set(*newspaper_article.keywords),\n",
        "            stock_symbol=stock_symbol\n",
        "        )\n",
        "        return result\n",
        "\n",
        "    def to_json(self):\n",
        "        return json.dumps(self.__dict__, default=str)\n",
        "\n",
        "    def article_dict(self):\n",
        "        return json.loads(json.dumps(self.__dict__, default=str))\n",
        "\n",
        "    def prompt_builder(self, prompt: str = \"\", text=\"\"):\n",
        "        if text == \"\":\n",
        "            text = self.text\n",
        "        items = self.__dict__.items()\n",
        "        out_list = [prompt] if prompt != \"\" else []\n",
        "        for (item_1, item_2) in items:\n",
        "            if item_1 == \"text\":\n",
        "                item_2 = text\n",
        "            out_list.append(f\"---\\n{str(item_1)}\\n---\\n{str(item_2)}\\n\")\n",
        "        return \"\\n\".join(out_list)\n",
        "\n",
        "    def update_text(self, text: str):\n",
        "      self.text = text\n",
        "      return self\n",
        "\n",
        "    def prepare_embeddable(self):\n",
        "        self.embeddable = preprocess(self.text)\n",
        "\n",
        "    @property\n",
        "    def metadata(self):\n",
        "        metadata = dataclasses.asdict(self)\n",
        "        del metadata['keywords']\n",
        "        del metadata['tags']\n",
        "        del metadata['embeddable']\n",
        "        metadata['publish_date'] = self.publish_date.isoformat()\n",
        "        metadata['search_date'] = self.search_date.isoformat()\n",
        "        return metadata\n",
        "\n",
        "################################################################################################################################################################################################\n",
        "\n",
        "class AsyncPostgresDB:\n",
        "    def __init__(self, database_url):\n",
        "        self.database_url = database_url\n",
        "        self.conn = None\n",
        "\n",
        "    async def _ensure_connection(self):\n",
        "        \"\"\"\n",
        "        Ensure that the database connection is established.\n",
        "        \"\"\"\n",
        "        if self.conn is None or self.conn.is_closed():\n",
        "            self.conn = await asyncpg.connect(self.database_url)\n",
        "\n",
        "    async def check_key_exists(self, table_name, column_name, key_value):\n",
        "        \"\"\"\n",
        "        Asynchronously check if a key exists in a specific table and column.\n",
        "\n",
        "        Args:\n",
        "        - table_name (str): The name of the table.\n",
        "        - column_name (str): The name of the column.\n",
        "        - key_value (str/int): The value to check for.\n",
        "\n",
        "        Returns:\n",
        "        - bool: True if the key exists, False otherwise.\n",
        "        \"\"\"\n",
        "        await self._ensure_connection()\n",
        "        query = f\"SELECT EXISTS(SELECT 1 FROM {table_name} WHERE {column_name} = $1);\"\n",
        "        result = await self.conn.fetchval(query, key_value)\n",
        "        return result\n",
        "\n",
        "    async def close(self):\n",
        "        \"\"\"\n",
        "        Close the database connection if it's open.\n",
        "        \"\"\"\n",
        "        if self.conn:\n",
        "            await self.conn.close()\n",
        "            self.conn = None\n",
        "\n",
        "\n",
        "################################################################################################################################################################################################\n",
        "\n",
        "def debugprint(*args, **kwargs):\n",
        "    if enable_debugprint:\n",
        "        print(*args, **kwargs)\n",
        "\n",
        "def chunk_list(input_list, n):\n",
        "    avg = len(input_list) / float(n)\n",
        "    chunks = []\n",
        "    last = 0.0\n",
        "\n",
        "    while last < len(input_list):\n",
        "        chunks.append(input_list[int(last):int(last + avg)])\n",
        "        last += avg\n",
        "\n",
        "    return chunks\n",
        "\n",
        "async def acheck_articles(session, url: str, i: int, total: int, postgres: AsyncPostgresDB) -> InputArticleRaw | None:\n",
        "    debugprint(f\"Processing {i+1}/{total}\")\n",
        "    try:\n",
        "        async with session.get(url, timeout=10) as response:\n",
        "            debugprint(\"Fetching article...\")\n",
        "            news_resolve = str(response.url)\n",
        "            debugprint(\"Article found.\")\n",
        "            article = Article(news_resolve)\n",
        "            article.download()\n",
        "            article.parse()\n",
        "            debugprint(\"Article parsed.\")\n",
        "            custom_uuid_to_check = custom_uuid(stock_symbol, article.title)\n",
        "            key_exist = await postgres.check_key_exists(table_name=\"langchain_pg_embedding\", column_name=\"custom_id\", key_value=custom_uuid_to_check)\n",
        "            if key_exist is True:\n",
        "                raise ValueError(\"Article already exists in embeddings database.\")\n",
        "            if type(article.publish_date) is not datetime.datetime:\n",
        "                raise ValueError(\"Publish_date not of type datetime.\")\n",
        "            if \"Are you a robot?\" not in article.title and \"403 Client Error\" not in article.text:\n",
        "                if article.text == \"\":\n",
        "                    raise ValueError(\"article text is empty string.\")\n",
        "            debugprint(f\"Appending article: {article.title}\")\n",
        "            return InputArticleRaw.import_newspaper(article, stock_symbol=stock_symbol)\n",
        "    except Exception as err:\n",
        "        debugprint(f\"Encountered error: {err}\\nargs: {session, url, i, total}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "async def aget_valid_articles(list_gnews_articles: List[str], postgres: AsyncPostgresDB, max_articles: int = 0) -> List[InputArticleRaw]:\n",
        "    max_articles = len(list_gnews_articles) if max_articles == 0 else max_articles\n",
        "    tasks = []\n",
        "    article_list = []\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        for i, news in enumerate(list_gnews_articles):\n",
        "            if len(article_list) >= max_articles:\n",
        "                debugprint(\"Max list length reached.\")\n",
        "                break\n",
        "            news_url = f\"https://{news}\"\n",
        "            debugprint(f\"news_url: {news_url}\")\n",
        "            task = acheck_articles(session, news_url, i, len(list_gnews_articles), postgres=postgres)\n",
        "            tasks.append(task)\n",
        "        articles = await asyncio.gather(*tasks)\n",
        "\n",
        "    for article in articles:\n",
        "        if article:\n",
        "            article_list.append(article)\n",
        "    return article_list"
      ],
      "metadata": {
        "id": "Ekwy22dEEGwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_news_list = chunk_list(news_list, 10)\n",
        "tasks = []\n",
        "enable_debugprint = True\n",
        "\n",
        "\n",
        "db = AsyncPostgresDB(connection_string)\n",
        "chunked_valid_news_lists = await tqdm_asyncio.gather(*(aget_valid_articles(list_gnews_articles=newslist, postgres=db) for newslist in chunked_news_list))\n",
        "await db.close()\n",
        "article_list = flattened_list = list(itertools.chain.from_iterable(chunked_valid_news_lists))"
      ],
      "metadata": {
        "id": "ZKL91jRGjgPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_splitter(chunk_size: int=512, chunk_overlap: int=0):\n",
        "    return RecursiveCharacterTextSplitter(\n",
        "        # Set a really small chunk size, just to show.\n",
        "        chunk_size = chunk_size,\n",
        "        chunk_overlap  = chunk_overlap,\n",
        "        length_function = len,\n",
        "    )\n",
        "\n",
        "chunk_size = model_context_window\n",
        "text_splitter = generate_text_splitter(chunk_size=chunk_size)\n",
        "\n",
        "my_llm = await summary_engine()\n",
        "\n",
        "summary_chain = load_summarize_chain(llm=my_llm, chain_type=\"map_reduce\")\n",
        "\n",
        "text_limit_prompt = PromptTemplate.from_template(f\"Take the below text and summarize it such that it still accurately conveys the same information, while falling below {chunk_size} tokens:\\n{{article}}\")\n",
        "text_limit_chain = LLMChain(\n",
        "    prompt=text_limit_prompt,\n",
        "    llm=my_llm\n",
        ")\n",
        "\n",
        "def summarize_article(text: str, summary_chain=summary_chain, text_splitter=text_splitter, chunk_size=chunk_size):\n",
        "    debugprint(f\"Summarizing article.\")\n",
        "    if len(text) <= chunk_size:\n",
        "        doclist = text_splitter.create_documents([text,])\n",
        "        result = summary_chain(doclist, return_only_outputs=True)\n",
        "        result = result.get(\"output_text\", result)\n",
        "        if len(result) > chunk_size:\n",
        "            result = text_limit_chain(inputs={\"article\":result}, return_only_outputs=True)\n",
        "            result = result.get(\"output_text\", result)\n",
        "    else:\n",
        "        result = text\n",
        "    if type(result) is not str:\n",
        "        raise ValueError(\"InputArticleRaw.text cannot be assigned non str value.\")\n",
        "    debugprint(f\"Article summarization complete.\")\n",
        "    return str(result)"
      ],
      "metadata": {
        "id": "QlOSo5O32dKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pg_embedding_text_splitter = generate_text_splitter(chunk_size=embedding_context_window)\n",
        "\n",
        "sumarry_generator = [article.update_text(summarize_article(article.text)) for article in tqdm(article_list)]\n",
        "for article in tqdm(article_list):\n",
        "    article.prepare_embeddable()\n",
        "summary_generator_w_progress = tqdm(sumarry_generator, total=len(article_list))\n",
        "summary_text_generator = [pg_embedding_text_splitter.create_documents(texts=[article.embeddable,], metadatas=[article.metadata,]) for article in sumarry_generator]\n",
        "flattened_summary_text = [document for doc_list in summary_text_generator for document in doc_list]"
      ],
      "metadata": {
        "id": "Fckevaex5jM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Ensuring embeddings are loaded.\")\n",
        "my_embeddings = await embeddings()\n",
        "print(\"Embeddings are loaded.\")\n",
        "test_embedding = my_embeddings.embed_query(\"test\")\n",
        "embedding_dimensions = len(test_embedding)\n",
        "print(f\"Embedding model produces vector of size {embedding_dimensions}\")\n",
        "\n",
        "db = PGEmbedding(\n",
        "    embedding_function=my_embeddings,\n",
        "    collection_name=stock_symbol,\n",
        "    connection_string=connection_string\n",
        ")\n",
        "\n",
        "for each in tqdm(flattened_summary_text, total=len(flattened_summary_text)):\n",
        "    text = each.page_content\n",
        "    metadata = each.metadata\n",
        "    title = metadata.get(\"title\", ValueError(\"Title missing from metadata.\"))\n",
        "    id = custom_uuid(stock_symbol, title)\n",
        "    db_connect = db.connect()\n",
        "    db.add_texts(\n",
        "        texts=[text,],\n",
        "        metadatas=[metadata,],\n",
        "        ids=[id,]\n",
        "    )\n",
        "    db_connect.commit()\n",
        "print(\"All documents in run are embedded.\")"
      ],
      "metadata": {
        "id": "XENx2585lY2E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}